\documentclass{article} % For LaTeX2e
\usepackage{iclr2017_conference,times}
\usepackage{url}
\usepackage{amsthm}
\input{preamble.tex}


\title{Neuron as an Agent}

\author{Shohei Ohsawa, Kei Akuzawa \& Yutaka Matsuo \\
The University of Tokyo\\
7 Chome-3-1 Hongo, Bunkyo, Tokyo \\
\texttt{ohsawa@weblab.t.u-tokyo.ac.jp} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
The reason why swarm of agents solve real-world problem well is, interestingly,
same as the principle of representation learning: good representation improves
perforamance of machine learning. Most of the problem in real-world is not
Markov decision process (MDP) but partially observed MDP (POMDP). On the
POMDP environment, good observation yields good action. In this paper, we
optimise a deep neural network as a multi-agent system as a natural extension
from representation learning to multi-agent reinforcement learning for POMDP.
To achieve that, we propose a novel learning framework, neuron as an agent
(NaaA). In NaaA, an individual unit is considered as an agent, and they maximizing
profit instead of minimizing error. To prevent dillemma, we borrow idea
from machanism design, a field of game theory. To this end, we show all the unit
have valid price reflecting their contribution to performance at convergence. We
confirm the result by numerical experiment using Atari and VizDoom.
\end{abstract}

\input{introduction.tex}
\input{related.tex}
\input{method.tex}
\input{experiment.tex}
\input{discussion.tex}
\input{conclusion.tex}
\input{appendix.tex}

\bibliography{daisy}
\bibliographystyle{iclr2017_conference}

\end{document}

