\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{url}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\input{preamble.tex}


\title{Neuron as an Agent}

%\author{Shohei Ohsawa, Kei Akuzawa, Yusuke Iwasawa \& Yutaka Matsuo \\
%The University of Tokyo\\
%7 Chome-3-1 Hongo, Bunkyo, Tokyo \\
%\texttt{ohsawa@weblab.t.u-tokyo.ac.jp} \\
%}
\author{Anonymous}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
We propose {\em Neuron as an Agent} (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.
NaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.
First, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.
To resolve that difficulty, we introduce a mechanism from game theory.
As a theoretical result, we demonstrate that the agent obeys the system to maximize its {\em counterfactual return} as the Nash equilibrium of the mechanism.
Subsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.
We propose {\em adaptive dropconnect}, a natural extension of dropconnect.
Finally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.
Specifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.
\end{abstract}

\input{introduction3-en.tex}
\input{related-en.tex}
\input{background-en.tex}
\input{method-en.tex}
\input{optimization-en.tex}
\input{experiment.tex}
\input{discussion-en.tex}
\input{conclusion-en.tex}
\input{appendix.tex}

\bibliography{daisy}
\bibliographystyle{iclr2018_conference}

\end{document}

