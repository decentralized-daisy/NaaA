\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{url}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\input{preamble.tex}


\title{Neuron as an Agent}

%\author{Shohei Ohsawa, Kei Akuzawa, Yusuke Iwasawa \& Yutaka Matsuo \\
%The University of Tokyo\\
%7 Chome-3-1 Hongo, Bunkyo, Tokyo \\
%\texttt{ohsawa@weblab.t.u-tokyo.ac.jp} \\
%}
\author{Anonymous}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Communication methods on multi-agent reinforcement learning (MARL) so far relies on a trusted third party (TTP) which distributes incentive to agents, and hence cannot be applied into a peer-to-peer environment.
We propose {\em Neuron as an Agent} (NaaA) for incentive distribution in MARL without TTP with two key ideas:
(i) inter-agent reward distribution and (ii) auction theory.
The reason why we introduce auction theory is inter-agent reward distribution is insufficient for optimization.
An agent in NaaA maximizes its profit, difference between reward and cost.
As a theoretical result, we show that the auction mechanism has an agent autonomously evaluates counterfactual return as other agent's value.
NaaA enables us to trade of representation in peer-to-peer and regard a unit in a neural network as an agent ultimately.
Finally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.
Specifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.
\end{abstract}

\input{introduction-en.tex}
\input{problem.tex}
\input{method-en.tex}
\input{optimization-en.tex}
\input{related-en.tex}
\input{experiment.tex}
\input{application-en.tex}
\input{conclusion-en.tex}
\bibliography{daisy}
\bibliographystyle{iclr2018_conference}

\input{appendix.tex}

\end{document}
