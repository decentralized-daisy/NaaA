\section{Introduction}
Deep reinforcement learning (DRL) succeeds in many areas.
Deep Q-Network (DQN) \citep{mnih2015human,silver2016mastering} finds the optimal action from a screen sequence from Atari, and selects the move closest to win from a face of a board of Go.
Deep Deterministic Policy Gradient (DDPG) \citep{lillicrap2015continuous} realizes the multiple-join control considering conditions such as friction and gravity factors in a physical space.
The applicability of DRL is becoming wider year by year. Reasonable performance is reported for 3D games such as Doom \citep{dosovitskiy2016learning}.

A neural network is workable for DRL because a neural network abstracts the implicit state in an environment and obtains an informative state representation.
From a micro perspective, the abstraction capability of each unit contributes to the return of the entire system.
Therefore, we address the following question.

\begin{center}
{\em Will reinforcement learning work even if we consider each unit as an autonomous agent?}
\end{center}

The contribution of this paper is that we propose {\em Neuron as an Agent} (NaaA) as a novel framework for RL, and its optimization method.
NaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.
In the of NaaA reward design, a unit distributes its received reward to other input units, passing its activation to the unit as cost.
Consequently, the actual reward is profit, defined as the difference between inflow (received reward) and outflow (paid cost).
In the setting, the economic metaphor can be introduced: profit is the balance of revenue and cost. 
Therefore, a unit should address tradeoffs between optimization of cumulative revenue maximization and cumulative cost minimization.

This paper is organized as presented below.
First, showing the optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider units as agents.
As a solution to this difficulty, we introduce a mechanism of auction which applies game theory.
As a theoretical result, we demonstrate that the agent obeys to maximize its {\em counterfactual return} as the Nash equilibrium.
The counterfactual return is that by which we extend counterfactual reward, the criterion proposed for multi-agent reward distribution problem \citep{agogino2006quicr}, along a long time axis.

Subsequently, we present that learning counterfactual return leads the model to learning optimal topology between the units.
In addition, we propose {\em adaptive dropconnect}, a natural extension of dropconnect \citep{wan2013regularization}.
Adaptive dropconnect combines dropconnect, which pure-randomly masks the topology, with adaptive algorithm, which prunes the connection with less counterfactual return with higher probability.
It uses $\varepsilon$-greedy as a policy, and is equivalent to dropconnect in the case of $\varepsilon = 1$. It is equivalent to counterfactual return maximization, which constructs the topology deterministically in the case of $\varepsilon = 0$.

Finally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.
Specifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.

Although considering all the units as agents might be simplistic at first glance, it has a wider applicable area.
From the perspective of optimization for a single neural network, it can be applied to pruning by optimizing the topology.
Furthermore, introducing the concept of reward distribution divides the single neural network to numerous autonomous parts.
It enables us not only to address sensor placing problem in IoT for partially observed Markov decision process (POMDP): arbitrary incentivized participants can join the framework.
 
