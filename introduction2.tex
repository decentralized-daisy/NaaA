\section{Introduction}
%=================================
%・マルチエージェントで観測を行うことによって、観測の精度を高める研究が行われている。
%　　　・マルチエージェントによる強化学習が現実の課題に対して有効である理由は、興味深いことに表現学習の原理と一致する：有用な表現は予測性能を向上させる。
%	・IoT、ビットコインなどの自律分散系において、センシング、分析においてそれぞれのレイヤーを分担することに必要である。
%・従来研究で解決されていない問題は、エージェント間が利己的であると仮定した場合の報酬の分配方法である。
%・本研究では、ゲーム理論の一分野であるメカニズムデザインの知見を応用し、最適な報酬の分配を実現する。
%	・エージェントの商品を複製可能な商品だとみなし、Digital goods auction の知見を適用する。
%・NaaA を提案する。
%	・次の前提からスタートする「環境の観測について、エージェントとユニットの役割は等価である。」
%	・特徴
%		・ユニットがどのレイヤーに属していても、平等に報酬を分配できる。
%・実験では、マルチエージェントの設定を行う。
%　	・Doom を用いて疑似的に再現したセンサーネットワークで、エージェントが協調行動を取ることを示す。
%	・従来研究では報酬が最適に割り当てられていなかった問題に対して、今回は割り当てられていることを確認する。
%================================-

マルチエージェント強化学習が現実の課題に対して有効である理由は、
興味深いことに表現学習の原理と一致する：有用な表現は予測性能を向上させる。
現実にある問題の多くは、DQN が前提としているマルコフ決定過程(MDP)ではなく部分的観測マルコフ決定過程(POMDP)である\citep[p.258]{sutton1998reinforcement}。
POMDP において真の状態は完全には不可視であり、良質な観測結果が良質な行動に結びつくため、有限のリソースを使って何を観測するかの設計が必要である。
必然的に、観測に用いられるエージェントが多いほど、将来的に獲得できる報酬が高いアクションを取りやすくなる。
今後、IoT やブロックチェーンといったブレークスルーにより、自律分散化したセンサーネットワークが出現すると考えられ、
環境に直接作用して報酬を獲得するエージェント以外に、
センサーを使って観測するエージェント、得られたデータを分析するエージェントなどが出現すると考えられる。
これらのエージェントを協調動作させ環境の観測範囲を広げることで、より実環境の問題を解きやすくなると考えられる。
%報酬の分配が実現できることで、自律的なエージェントが参入してくる可能性があると考えらえる。
%我々は、IoT や FinTech といった社会的要請にモチベートされている。

マルチエージェントが観測した情報を活用してより精度の高い方策の決定を行うために、
エージェント間を行き来する信号を工夫するコミュニケーションモデルの研究が重要である。
CommNet \citep{sukhbaatar2016learning} は、エージェントが受け渡し合う信号にベクトルを用い、
有用な信号をバックプロパゲーションによって学習することを可能にしている。
これは、マルチエージェントシステム全体を一つのニューラルネットワークとみなしているのと同義である。
彼らの手法は、すべてのエージェントは協力的で、一つの目的関数を最大化するという仮定を置いている。


CommNet のようなコミュニケーションモデルが解決していない問題は、
情報の重要性に応じた報酬の分配であり、信頼度割り当て問題として知られている。
各エージェントが同一報酬を受け取る設定は、
有用な情報を提供しなくても報酬を受け取るフリーライダーの出現という問題を持つ。
これは、エネルギーや通貨のように、エージェント全体が獲得できる報酬の総和が限られている場合に、よりシビアな問題となる。
なぜなら、フリーライダーの存在が、他のエージェントの報酬を減らすためである。
一般に、ジレンマの存在により、マルチエージェントシステムとしてニューラルネットワークを構築することは容易ではない。
ジレンマとは、個別最適であるナッシュ均衡が全体最適であるパレート効率性を達成しない問題である。
フリーライダー問題は、最終的に系がノイズのみを提供するエージェントで埋め尽くされる、コモンズの悲劇と呼ばれるジレンマを誘発する。
%第一に、
%第二に、ユーザがどのようにコモンズを受け取るのかという問題が存在する。

本研究では、報酬の分配にゲーム理論におけるメカニズムの一つである digital goods auction \citep{guruswami2005profit} を用いることで、自然にパレート効率性が達成されることを示す。
ナッシュ均衡として、ユニットは予測誤差の最小化の代わりに、そのユニットが存在する場合と存在しない場合の系全体のリターンの差である counterfactual return を最大化する。
これは、マルチエージェントシステムの報酬分配問題に対して提案されている指標である counterfactual reward \citep{agogino2006quicr} を時間方向に拡張したものである。

%実験では、マルチエージェント系を拡張し、Doom の環境で複数のエージェントを動作させることで、実験の精度が高まることを示す。
%本研究ではマルチエージェントのジレンマ問題の解決にメカニズムデザインを応用する。

本研究のゴールは、すべてのユニットが自律的に動作すると仮定した場合に、
システム全体が獲得する累積報酬（リターン）を最大化することである。
これを、新しいタスクである {\em Neuron as an Agent} (NaaA) として定式化する。
NaaA では、以下の前提を設ける。

\begin{center}
環境の観測について、エージェントとユニットの役割は等価である。
\end{center}

すなわち、パーセプトロンや Convolutional Net (CNN) などの微分可能(differentiable)な関数で構成されるニューラルネットワークの個々のユニットを、自身のリターンを利己的に最大化するエージェントであると仮定する。
%NaaA のユニットは、このように共通の値を最適化するということを陽に目的としない。
%そのため、CommNet のようなマルチエージェントシステムのように中央集権的なコントローラを持たず、
%環境に存在する複数のエージェントが自律的に動作するという特徴を持つ。
通常、ニューラルネットワークの目的は、目標値と予測値の間の誤差などの、共通の criterion $e$ の最小化である。
そのため、各ユニットはバックプロパゲーションによって出力 $y$ による微分 $\partial e / \partial y$ を計算し、$e$ が小さくなる方向に $y$ の大きさを制御していた。
NaaA では、メカニズムデザインの帰結として、各エージェントが自身の counterfactual return を最大化する。

NaaA のユニットは、counterfactual return の期待値を予測するための
{\em valuation net} を持つ。
すなわち、系全体は各ユニットがニューラルネットワークを持つ入れ子構造をしている。
本論文では、valuation net の構成例や、強化学習を通して訓練する方法を述べている。

実験では、標準的な強化学習のタスクによる数値実験を用いて NaaA が POMDP の問題の精度を高めることを示す。
具体的に、Atari および VisDoom における環境を用いて、既存研究が DQN や A3C を上回ることを示す。

%マルチエージェントにおける QUICR 学習では counterfactural reward に基づく報酬の分配方法を提案しているが、deep neural network のような階層的なネットワークに最適化された手法ではない。
%そのため、中央集権的でないエージェントはどのように
