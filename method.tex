\section{Neuron as an Agent}
本セクションでは、ニューラルネットワークの構造について復習(recap)した後、
報酬分配のフレームワークである profit maximization について述べ、
Valuation Net について説明を行う。

神経回路に含まれるニューロンは一つの細胞であるため、エネルギーを消費する。
通常の細胞と同様に酸素や ATP がエネルギー源となり、これらはニューロンと接続した、アストロサイトから供給される。
アストロサイトは脳の構造を支えるグリア細胞の一種であり、血管からニューロンへの栄養供給を行う。
エネルギー量は有限であるため、不要なニューロンはアポトーシスによって死滅する。
アポトーシスは NGF, BDNF などの神経栄養因子(NTF)によって制御されるため、より多くの NTF を獲得できたニューロンが生存する。
%そのため、NTF が一種の通貨として機能する。

こうした NTF による仕組みを選択圧としてとらえるニューラルダーウィニズムという考え方がある。
ニューラルダーウィニズムでは、一つのニューロンを、バクテリアのような生物学における個体であるかのように扱う。
環境に適応したニューロンが生存し、そうでないニューロンが死滅することで、
有用なニューロンが生き残るようにする。

NaaA では、こうしたニューロンによる栄養追及の枠組みを強化学習でモデリングする。
強化学習はエージェントと環境の相互作用を扱う。
エージェントは環境に対して一つの状態を持ち、環境に対してアクションを行うことで状態遷移を行い、報酬を受け取る。
アクションはエージェントの持つ状態とアクションの間の確率分布、方策を持つ。
強化学習の目的は、長期的なエージェントの受け取る報酬の未来にわたる減衰和を最大化することである。
NaaA では、ニューロンを一つのエージェントとみなし、環境を接続している他のニューロンと仮定する。

エージェント間の通信は一つのマシン内で行われてもよいし、マシン間で行われてもよい。
%以下では、ニューロン、ユニット、エージェントは断りのない限りすべて同じ意味で用いることにする。

ニューラルネットワークにはパーセプトロン、ボルツマンマシンなど様々なものが存在するが、
NaaA が対象とするニューラルネットワークの種類は、
多層パーセプトロン、CNN、RNN、LSTM のような、微分可能(differentiable)ニューラルネットワークである。
これは、出力をパラメータで微分可能な関数群で構成される。
直感的には、2017 年現在の TensorFlow でサポートされるものすべてを指す。
一方で、ボルツマンマシンのような確率ベースのものは今回は対象としない。
%これらは 2017 年現在の TensorFlow, Pylearn, Torch, Caffe など多くのツールでサポートされている。
%これらはバックプロパゲーションによってパラメータの最適化、すなわち学習をすることができる。

微分可能ニューラルネットワークは、本来は人間の神経回路を模して造られたものであり、
神経を構成するニューロンをユニットという形で抽象化している。
ユニットは互いにつながっており、これはニューロンの軸索に対応付けられる。
ユニットは接続されたユニットから受け取った信号に対して反応し、信号を出力する。


% ユニットの計算方法
ユニットの計算方法は、既存のニューラルネットワークとほとんど同じである。
ユニットは、一つのスカラー値を信号として出力する。
この信号は、他のユニットから受けた入力に基づき計算される。
線形演算、すなわち重み付き和を計算するユニットもあれば、
ReLU のような活性を計算するユニットも存在する。

ユニットを一つのエージェントとみなすことで、最適化する対象の問題はマルチエージェント強化学習とみなすことができる。
マルチエージェント強化学習における問題とは、環境が与えた報酬 $R_0$ を複数のエージェント間でどのように分配するかにある。
%強化学習における信頼度割り当て

%NaaA では、ユニットを次の性質を満たすエージェントであるいう前提から出発する。
NaaA は、$N$ 個のエージェントで構成されるマルチエージェントシステムである。各ユニットは本来の性質に加えて、以下の性質を持つ。
\begin{enumerate}
\renewcommand{\labelenumi}{N\arabic{enumi}:}
\item （利己性）エージェントは、系全体の期待リターン $G_{0t}$ ではなく、自身の期待リターン $G_{it}$ の最大化を目的として行動する。
\item （報酬の分配）各エージェントが受け取る報酬 $R_i$ の総和は、マルチエージェントシステム全体が外的環境から得る報酬 $R_0$ に等しい。
\item （NOOP）エージェントは、期待リターンが 0 の NOOP (no operation) という行動をオプションとして持つ。NOOP では、エージェントは何も入力せず、何も出力しない。
\item （コスト）エージェントには、生存コスト $\alpha_i$ が存在し、タイムステップごとに負の報酬として消費される。NOOP の状態では、生存コストは消費されない。
\item （取引）エージェント間の信号の送受信は取引である。すなわち、エージェントは信号を他のエージェントに伝達する際に、信号と引き換えに報酬を受け取る。
\item （Envy-free性）信号を同時に複数のエージェントが購入する場合、それはすべて同じ価格で取引される。すなわち、エージェントは同じ時間(timing)に一つの価格(price)を持つ。
\end{enumerate}
3 つめの仮定は、ニューロンのアポトーシスに相当する。
NOOP が選択されるのは、それ以外のすべての行動の期待報酬が負であった場合である。

%そこで、もう一つ前提を増やす。

\subsection{Profit Maximization Framework}
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{img/double.eps}
\caption{
本研究では、digital goods auction の枠組みを用いて、報酬の最適配分を実現する。
}
\label{fig:double}
\end{figure*}

これまでのニューラルネットワークの目的は、予測誤差 $e$ にあった。
そのため、各ユニットはバックプロパゲーションによって $\partial e / \partial y$ を受け取り、
$e$ が小さくなる方向に $y$ の大きさを制御していた。
NaaA において、エージェントの目的は大域的誤差の最小化ではなく、エージェント自身の利益の最大化である。
ここで、利益とは、エージェントが受け取る売上と、支払うコストの差である。
すなわち、各エージェントは次の関数を最大化する。
\begin{flalign}
R_t = r_t - c_t
\end{flalign}

エージェントの目的関数は次のように表現される。
\begin{flalign}
\Expect{\pi}{\sum_{i=0}^{T} \gamma^i (r_{t+i} - c_{t+i})}
&= \Expect{\pi}{\sum_{i=0}^{T} \gamma^i r_{t+i}} -  \Expect{\pi}{\sum_{i=0}^{T} \gamma^ic_{t+i}} \\
&= G_t^{\mathrm{in}} - G_t^{\mathrm{out}}
\end{flalign}
ただし、$G_t^{\mathrm{in}} \equiv \Expect{\pi}{\sum_{i=0}^{T} \gamma^i r_{t+i}}$、
$G_t^{\mathrm{out}} \equiv \Expect{\pi}{\sum_{i=0}^{T} \gamma^ic_{t+i}}$ である。
この式は、エージェントが終端状態までに生み出した付加価値に等しい。
これを行うために、エージェントは売上の最大化とコストの最小化を同時に行うことがわかる。

この式を通常の枠組みに沿って解くと、コスト最小化によってエージェントは他のエージェントに対して金額を支払わないことが正解となる。
そのため、すべてのエージェントがフリーライダーとなり、全体としての報酬が下がるという、パレート劣位問題が発生する。
NaaA ではこれを回避するため、オークションの仕組みを用いて、同じユニットの出力先にあるユニットを競争させることによって最適価格を決定する。

説明を単純化するために、DQN のような $l$ 層からなるフィードフォワードネットワークを考える。
なお、任意のDAG 構造（時間方向に展開された RNN やホップフィールドネットワーク）でも同様のことを考えられるため一般性を失わない。
各エージェントが受け取る報酬の総和はが環境が与える報酬に対して等しい必要がある。すなわち、次の式が成立する。
\begin{flalign}
R_{S,t} = \sum_{i \in S} R_{it}
\end{flalign}
最適な分配方法としてはいくつか考えられるが、ここでは系全体からエージェントを取り除いた場合の期待損失 $D_{it}$ にを$R_{it}$ を近づけるという方法を取る\cite{needed}。
この方法では、すべてのエージェントが独立に報酬に貢献している場合に、$ \sum_{i \in S} D_{it} = R_{S, t}$ が成立する

今回は、制約条件として、報酬は信号同士の取引によって与えられるというものを加える。	% 天下り的。なぜ加えるかの説明がなっていない。
したがって、ニューロンは、そこで、アクチュエータ 0 と接続している複数のユニット集合 $F_0 \subset S$ を考える。
この時、アクチュエータは得られた報酬のうち、次のようにして各ノードへの分配を行う。
\begin{flalign}
R_{0t} = \sum_{i \in V_0} r_{it}
\end{flalign}
ただし、$r_{it}$ は貢献度に応じた報酬額である。
次に、ユニット $i$ は計算に必要なデータを「購入」するために、接続されている（すなわち $V_i$ に含まれる）他のユニットに対して、報酬を次のようにして分配することを考える。
\begin{flalign}
c_{it} = \sum_{j \in V_i} r_{jt} + \alpha_{jt}
\end{flalign}
ただし、$\alpha_i$ は $c_{it}$ が本質的に備えているコスト（観測コスト、計算コスト）である。
この仕組みを再帰的に繰り返し、入力層まで繰り返すと、次の式が成立する。
\begin{flalign}
R_{0t} = \sum_{i \in V_0} r_{it} = \sum_{i \in S} (r_{it} - c_{it}) 
\end{flalign}
したがって、$R_{it} = r_{it} - c_{it}$ が成立する。
%これは、報酬はすべてのノードが、下位のユニットから受け取った信号に対して与えた付加価値の合計に等しい。
この式は、$R_{0t}$ は、各ユニットが生み出した付加価値 $r_{it} - c_{it}$ の合計として表現されることを意味している。

この付加価値のことを、NaaA では経済学のメタファーを用いて、利益(profit)と呼ぶ。
利益は強化学習において追求の対象となる報酬(reward)に相当する。
同様に、$r_t$ を収益(revenue)、$c_t$ をコスト(cost)と呼ぶ。
すなわち、NaaA の枠組みでは、各エージェントは利益の最大化を目的として、収益最大化とコスト最小化を同時に行うことになる。

通常のニューラルネットワークの枠組みを用いて最適化を行うと、
すべてのエージェントが支払うコストは 0 に収束する。
したがって、この最適化問題の自明なナッシュ均衡解として得られるのは、
すべてのユニットに対する報酬が 0 であり、アクチュエーターのみが報酬を獲得するという状況である。
これは認知や入力を軽視した従来の状況と合致する。

従来はこれでもよかったが、入力にコストが生じる場合、パレート劣位な状況が生じる。
すなわち、入力ユニットに対して報酬が与えられないと、入力側のインセンティブがなくなり、$R_{it} = -\alpha_{it}$ となる。
この場合、入力ユニットは観測をしない方がよいという判断になり、出力を停止する。
その結果、アクチュエーター側では、行動の決定に必要なデータがいきわたらなくなり、報酬が低くなる。
こうした状況を避けるために、期待報酬が負である場合、エージェントは行動しない。

これを避けるために、NaaA ではオークションの仕組みを用いて資源の最適配分を行う。

%したがって、次の式が成立する必要がある。
%これを変形すると、次のようになる。
%\begin{flalign}
%R_{jt} = R_{Nt} - \sum_{i=1, i \ne j}^n R_{it} = R_{Nt} - R_{N-\{j\}, t}
%\end{flalign}
%直感的には、$R_{jt}$ は N から $j$ を除いた系で得られる報酬に等しいため、エージェント $j$ が与える付加価値としてとらえることができる。
%\begin{flalign}
%\sum_{i=1}^n r_{it} = \sum_{i=1}^n c_{it}
%\end{flalign}

%g% 何が異なるか
%gNaaA におけるユニットがこれまでのユニットと異なる点は、
%gユニット間の通信が信号の一方的な流れではなく、取引、
%gすなわち信号と資金の交換である点である。
%g%ユニットが他のユニットから信号を受け取る際は、資金を払う必要がある。
%g
%g説明を簡単にするために、最初に 2 つのユニットで構成される売り手-買い手モデル(seller-buyer model)を考える。
%gこのモデルは、売り手のユニットが信号を計算し、計算した値に対して値段を付けて買い手に対して提示(ask)する。
%g買い手は、買取価格を入札(bid)する。もし、希望価格(ask price)が入札価格(bid price)を下回れば、希望価格と入札価格の間の価格で合意し、買い手は売り手に対して価格を考える。
%gSeller-buyer モデルの問題は、買い手が値切れてしまうことである。
%gこれは対戦略性を持たないと呼ばれる。
%g希望価格を決定する根拠を持たない点にある。
%g
%gNaaA では、ユニットの出力を、デジタル財、すなわち、供給が無限に存在する財とみなした資源の最適配分について考える。
%gメカニズムデザインでは、環境は $(N, \Theta, 0)$ で表現される。
%g
%gここで重要なのは、最適な価格の決定方法である。
%g次に、一つのユニットと複数のユニットからなるモデル、seller-buyers モデルを考える。
%g一つのユニットは、複数のユニットに対して、情報を売る。
%g
%gユニットは入力ユニットに対して、
%g取引は、ユニットは他のニューロンに対して定価
%g出力値は、他のユニットに対してそのユニットの定価 $a_it$ で売却する
%gしたがって、

%========================================
% 売上
%========================================

ここで、売上とコストの選定方法について分けて説明を行う。
まず、売上について考える。エージェントは利益を最大化したいため、利益は次のようにして与えられる。
\begin{flalign}
	r_{it} = \max_{ a \ge 0 } a d_t(a) 
\end{flalign}
ここで、$a$ は価格、$d_t(a)$ はユニット $i$ の信号に対する価値を $a$ 以上と評価しているエージェントの数であり、需要(demand)と呼ぶ。
同様の式で、右辺を最大化する $a$ を最適価格と呼び、$ \hat{a}_{it} $ で表す。 


%========================================
% コスト
%========================================

次に、コストについて考える。
エージェントが他のエージェントに対して支払うコストは、
購入に成功した場合は売り手の提示価格 $\hat{a}_{it}$ を支払い、
そうでない場合は支払うコストは 0 になる。
コスト最小化の枠組みから考えると、支払うコストが 0 である方向を選ぶことがあるが、
エージェントは常にデータを購入しない選択をする可能性がある。
しかし、これは潜在的には将来の売り上げを棄損していることになる。
データを購入せず、劣悪なデータを流した場合の売上の低下を考える。
これは、正しいデータを購入した場合とそうでない場合の長期 $r_{it}$ の差に等しい。
すなわち、
%FIXME 多分ここは期待値じゃなくてリターンそのものなのでそのように修正する（最初にリターンを使ったロジックで説明しているため）
\begin{flalign}
	o_t 
	&= \Expect{\pi}{ r_{i,t+1} \mid a_t=1 } - \Expect{\pi}{ r_{i,t+1} \mid a_t=0 } \\
	&= \Expect{\pi}{ R_{i,t+1} \mid a_t=1 } - \Expect{\pi}{ R_{i,t+1} \mid a_t=0 } \\
	&= Q(s_t, 1) - Q(s_t, 0) \label{eq:def:oppotunity-cost},
\end{flalign}
ただし、$Q$ は状態行動価値関数であり、一手先のコストはどちらの行動を選んでも一定であると仮定している。
$o_{it}$ を counterfractual state-action value と呼ぶ。これは QUICR \citep{agogino2006quicr} と導出は異なるが等価である。
すなわち、エージェントが支払うコストは、データの購入に成功した場合は $\hat{a}_{it}$ であり、
それ以外は $o_{it}$ となる。

では、コストを最小化するためのエージェントの入札額 $b_{it}$ は何か。
これについては次の定理が成立する。

\begin{thm}\label{thm:optimal-bidding}
コストを最小化する最適な入札額は $b_{it} = o_{it}$ である。
\end{thm}
証明については Appendix を参照。

すなわち、エージェントは自身の機会損失のみを問題にすればよい(!)
したがって、NaaA のメカニズムでは、エージェントはあたかも他のエージェントを価値評価(valuation)し、
その価値を正直に申告していることを意味する。

系として次の解が得られる。
\begin{coro}\label{coro:optimal-bidding}
The Nash equivalem of the envy-free game $(\vect{b}, q)$ is $(\vect{o}_t, \max_{q} q d_{\vect{o}_t}(q))$.
\end{coro}

%TODO o_t のグラウンドをする。最初に望ましい配分 v を述べ、その後で重要性について解説する。


\subsection{Valuation Net}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{img/network.eps}
\caption{
Valuationn Net は情報の価値を評価し、bidding price を決定する。
下位のニューロンに対して入札し、信号を購入する。購入したデータを用いて、データを次のニューロンに対して売る。
}
\label{fig:network}
\end{figure*}

残る問題は、$\vect{o}_t$ をいかに推定するかである。
この推定には様々な方法が存在しており、多くのメソッドを使うことができるが、
本論文では $Q$ の推定に $Q$-learning を採用する。
ただし、SARSA や actor-critic などの on-policy な方法も使うことができることを補足する。

図\ref{fig:network}に示すValuation Net は、通常のニューラルネットワークのユニットに、
$Q$-learning による valuation を組み合わせたネットワークである。
まず、上部はエージェント間の通信について示したものである。
ニューラルネットワークではユニットを円で表現するのが通例であるが、
ここではユニットをエージェントとしてみなすことを強調して、六角形で一つのユニットを表現している。
エージェント間では、通常のニューラルネットワークと同様の信号の通信以外に、
取引に関する通信(allocate, buy, sell \& bid)が発生する。

Valuation Net では、状態 $\vect{s}_t$ として、
予測後入力 $\tilde{\vect{x}}_t$ および入力に依存しない構成情報 $\vect{u}$ を横につなげたベクトル $(\tilde{\vect{x}}_t^\T, \vect{u}^\T)^\T$ を用いる。
構成情報の一例としてはユニットのパラメータがあげられ、たとえば重みやバイアスの情報を用いることができる。

状態からの Q 関数の予測にニューラルネットワークを用いる。
エージェントが受け取った売上に基づき時間差分(TD)-誤差 が計算され、
ネットワークが訓練される。
ネットワークの構成にはこれまでの deep Q-learning で用いられている二重化ネットワーク(dualing network) \citep{wang2015dueling} のテクニックを用いる。
オリジナルの文献\citep{wang2015dueling}で述べられている二重化ネットワークは、学習を加速するために、
状態関数と、Q関数との差分を別々に予測する手法である。
\cite{dosovitskiy2016learning} はこれに対して、差分の要素の挿話が 0 になるように正規化するよう改良している。
本研究では \cite{dosovitskiy2016learning} の手法に従い、
期待値 $E(\vect{s}_t)$ と正規化差分 $\tilde{A}(\vect{s}_t)$ を別々に求める。

$Q$関数は次のように表現される。
\begin{flalign}
	Q(\vect{s}_t, a_t) &= E(\vect{s}_t) + \tilde{A}(\vect{s}_t, a_t) \label{eq:QisE-A} \\
	\sum_{i+1}^k \tilde{A}_i(\vect{s}_t, a_t) &= 0
\end{flalign}
第2式を満たすために、まず、$\vect{s}_t$に基づいた予測を行い、次のような正規化を行う。
\begin{flalign}
	\tilde{A}_i(\vect{s}_t, a_t) &= A_i(\vect{s}_t, a_t)  - \frac{1}{k} \sum_{j=1}^k  A_j(\vect{s}_t, a_t)
\end{flalign}
次に、valuation を行い、bidding price $\vect{b}_t$ を求める。
$b_{it}$ の値は式 \ref{eq:def:oppotunity-cost} および式 \ref{eq:QisE-A}より、最適な入札価格 $\hat{b}_{it}$ は次のように計算できる。
\begin{flalign}
\hat{b}_{it} = \tilde{A}(\state_t, 1) - \tilde{A}(\state_t, 0)
\end{flalign}
Valuation Net ではこの式に基づき、advantage の出力を引き算することで入札価格を計算している。


%\subsection{Valuation Net}

% method
