\section{Introduction}
% 集合知的な話をした方がいい。
Collective intelligence enables an agent to make a decision preciselly through cooperation and communication among swarm of agents in partially-observed environment. 
The structure with collective intelligence is found everywhere in the nature such as animals, civilization and the Internet.
Recent emerging topics such as Internet of Things (IoT) and bitcoin \citep{nakamoto2008bitcoin} makes us to rethink the Internet to address continuous information and reward distribution through deep reinforcement learning (DRL).
IoT is vital to improve utility of agents by improving precision of actions by complementing observation, and hence expected to have potential applications such as automatic stock trade, smart grid, demand prediction for e-commerce and autonomous cars.
If the Internet and deep reinforcement learning are combined, 
explosive worldwide sensor data exploitation will be occured rather than process in a stand-alone computer.

Our motivation is addressing monetary reward distribution in collective intelligence as an instance of multi-agent reinforcement learning (MARL) with a neural network.
The goal is maximizing total cummurative reward of agents indirectly observing/affecting to an enviroment through communication and reward distribution.  %from the environment 
In the configuration, agents are trading their observation with own currency, and the cummurative reward is measured by amount of the remaining currency.
While several literature addressing multi-agent communication \citep{foerster2016learning,sukhbaatar2016learning}, they assumed all the agent receive the same reward from the environment, and hence cannot apply to our setting naively.
The problem is because of {\em social dilemma} also known as {\em prisoner's dilemma}, which bounds the potential cummurative reward since it encourages agents preferring betrayal than cooperattion against other agents if there are no central administrators.
The typical betrayal is hidden information. That is, buyer preferes to estimate underestimate value of observation for paying nothing, and the seller prefers to sell noise instead of observation. %TODO 何か処理をすること前提
Consequently, the communication will be sea of noise, and the reward from the environment will be minimized.

This paper brings envy-free auction \citep{guruswami2005profit} in economics to MARL, and makes agents to declare true information without administrator.
Envy-free auction is a basic mechanism in a digital goods auction, an auction mechanism for copiable goods such as books and music.
Before buying the data, the buyer bids price to observation of the seller, and the seller determines price which maximizes revenue itself from set of prices from multiple buyers.
Then, the seller sends its observation to buyers who bids price more than the asked price.
%With this mechanism, 

% この辺ちょっと甘い
Our method optimizes communication among agents with backpropagation similarly to CommNet \citep{sukhbaatar2016learning},
which regards an agent as a neuron.
Since the buyer/seller stucture among the agents can be broken down into a single unit ultimately, 
we can create a model which see a single unit as an agent, and hence we name our method {\em Neuron as an Agent} (NaaA).
NaaA regards a neural network as a multi-agent system, and determines bidding price with Q-learning.
This paper shows that in a framework of NaaA, the agents obey to maximize {\em counterfactual return}, which difference of return whether the data is observed or not.

This paper is organizes as following.
First, we show a framework and problem definition of NaaA, and proposed method with auction theory.
Next, we show two applications: the one is multi-agent setting with VizDoom and the anotehr one is optimization for topology of neural network.
After confirming the merit of NaaA, we introduce related works, discuss for further application, and conclude the paper.

%%=======================================================================================================================
%
%%When we naively apply the existing methods, 
%This paper modelize the situation with a model with agents indirectly observing the environment through communicating other agents.
%The other agents observes reward from the agent without direclly act in the environment.
%
%The model allows to have an agent which can observethe 
%
%If we naively apply the method in existing literatuere, it is known to 
%
%%This paper modelize the situation with a model with agents indirectly observing and making action to an environment.
%%Collective intelligence is one of the shapes of intelligence which enables an agent to make decision preciselly through cooperation and communication among swarm of agents in partially-observed environment, which 
%%If distributed reward system and deep reinforcement learning are unified, 
%%While the Internet accerelarated power of collective intelligence through search engines, it mainly addresses discrete information such as Web pages, and hence the connection to the area of neural network is thin so far.
%%explosive worldwide sensor data exploitation will be occured rather than process in a stand-alone computer.k
%
%%While the Internet showed power of collective intelligence of natural language by earch engines, 
%the trial is not achieved for continuous information such as sensor data, and most of companies are addressing the problem under the slogan of Internet of Things (IoT).
%In terms of reinforcement learning, as the real world is massive partially observed environment, IoT is vital to improve utility of agents by improving precision of actions by complementing observation, and hence expected to have potential applications such as automatic stock trade, smart grid, demand prediction for e-commerce and autonomous cars.
%Nonetheless, recent IoT has a problem that insufficient sensor data are collected in contrast to web pages
%since the utility to make the data public is inferior to the one making the data private.
%On the other hand, Blockchain system such as Bitcoin enabled us peer-to-peer reward distirbution among the agents,
%and is expected to realize the reward system in the brain.
%If distributed reward system and deep reinforcement learning are unified, 
%explosive worldwide sensor data exploitation will be occured rather than process in a stand-alone computer.
%Our motivation is addressing monetary reward distribution in collective intelligence as an instance of multi-agent reinforcement learning (MARL) with a neural network.
%
%
%a situation in which agents trade and exploit data with a situation in which 
%
%%micropayment to distribute reward finer than cent.
%%as the data provider are not incentivised.
%%Exploiting worldwide information by collecting it through the Internet is one of the challenges of intelligence.
%%While it has been almost done for natural language by biblioth\`{e}que and search engines, 
%%IoT enablesd us to share sensor data, and Blockchain enabled us peer-to-peer reward distirbution among the agents with Bitcoin.
