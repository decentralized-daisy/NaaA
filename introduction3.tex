\section{Introduction}
深層強化学習(DRL)は多くの領域で成功を収めている。
DQN \citep{mnih2015human,silver2016mastering} は、Atari のスクリーン系列から最適な行動を決定したり、AlphaGo のモジュールとして囲碁の盤面から勝利に最も近い一手を選ぶ。
DDPG \citep{lillicrap2015continuous} は物理空間において摩擦や重力係数などの条件を考慮した多関節の制御を実現する。
DRL の適用範囲は年々広がっており、Doom のような 3D ゲームにおいても一定の性能が報告されている\citep{dosovitskiy2016learning}。
%単一のエージェントを用いて強化学習を解くという試みは、人間の持つ身体性のアナロジーから考えると一見して妥当であるように思える。

% 問題について考える
%・コストの仕組みを考える。
DRL にニューラルネットワークが有効に機能するのは、ニューラルネットワークが環境の潜在状態を抽象化し、有用な状態表現を獲得しているためである。
これをミクロに考えれば、各ユニットの持つ抽象化能力が系全体のリターンに貢献していると考えられる。
%これは、それぞれのユニットが、自律的に系全体のリターンを最大化するような仕組みが構築可能であることを示唆している。
そこで、本研究では一つの問いを扱う。

\begin{center}
すべてのユニットを自律的なエージェントとみなした強化学習は成立するか？
\end{center}

本研究の貢献は、新しい強化学習のフレームワークとして {\em Neuron as an Agent} (NaaA) を提案し、その最適化方法を示すことである。
NaaA では、ニューラルネットワークにおけるすべてのユニットをエージェントとみなし、
報酬の分配をマルチエージェントの強化学習問題として最適化する。
NaaA の報酬設計では、ユニットは自身が受け取った報酬を、信号を入力している他のユニットに配分する。
したがって、報酬は、受け取る収益と、支払うコストの二つの差で定義される利益である。
そのため、ユニットは累積収益最大化と累積コスト最小化の両方を同時に行う。

%本論文は次のように構成される。
%まず、必要な前提知識と NaaA の定式化を示すと同時に、単純な最適化では、配分するコストが 0 になり、学習が不可能になるという否定的な結果を述べる。
本論文は次のように構成される。
まず、周辺の研究と NaaA の定式化を示すと同時に、単純にユニットをエージェントとみなしただけでは、逆に性能が下がるという否定的な結果を報告する。
%これは、コスト最小化の枠組みにより、すべてのコストが 0 に収束するという trivial な解が得られるためである。
そこで、この問題の解決方法として、ゲーム理論を応用したオークションの仕組みを導入する。
Theoretical result として、この仕組みにおけるナッシュ均衡として、エージェントは自身の counterfactual return を最大化するという解が得られることを示す。
これは、マルチエージェントシステムの報酬分配問題に対して提案されている指標である 
counterfactual reward \citep{agogino2006quicr} を時間方向に拡張したものである。

次に、counterfactual return の学習をすることが、ユニット間の最適なトポロジーを学習することにつながることを示し、
dropconnect \citep{wan2013regularization} の自然な拡張である {\em adaptive dropconnect} を提案する。
adaptive dropconnect は、単にランダムにトポロジーにマスクをかける dropconnect に、
counterfactual return の小さなものを優先して pruning するような学習を組み合わせたものである。
adaptive dropconnect では、$\varepsilon$-greedy を方策として使っており、$\varepsilon = 0$ の場合には dropconnect と等価に、$\varepsilon=1$ の場合には決定論的にトポロジーを構築する counterfactual return maximization と等価になる。

最後に、NaaA のフレームワークで最適化を行うことが強化学習の性能向上につながることを、
数値実験を通して検証する。具体的に、交通シミュレーションと、ViZDoom の実験環境を用いる。

すべてのニューロンをエージェントとみなすことは、一見無意味なように思えるが、実は広い応用範囲を持つ。
まず、報酬の分配という概念を導入することで、 センサー配置問題などの Partially Observed Markov Decision Process (POMDP) におけるを解決できるだけでなく、任意の参加者が仕組みに参加できるようになる。
単一のニューラルネットワークの最適化という観点から見れば、トポロジーの最適化を行うことで pruning に応用できる
ニューラルネットワークに報酬系という概念を導入することで、所有者に適切な報酬が配分できるようになるため、単一のニューラルネットワークを分散化できる。
%NaaA は、IoT、モバイル、ブロックチェーンといった産業上の技術と融和することでさらに価値を発揮すると考えらえる。

%強化学習における探索と活用のバランスを取っている。
%活用を重視した場合には決定論的にトポロジーを構築する場合と等価であり、探索を重視した場合には dropconnect と等価になる。
%$\varepsilon$-greedy のような方策を取ることにより、
