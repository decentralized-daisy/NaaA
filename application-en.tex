%\section{Application for Single Agent}
\section{Further Application}
One benefit of NaaA is that it can be used not only for MARL, but also for network training.
Typical neural network training algorithms such as RMSProp \citep{tieleman2012lecture} and Adam \citep{kingma2014adam} are based on sequential algorithms such as the stochastic gradient descent (SGD).
Therefore, the problem they solve can be interpreted as a problem of updating a state (i.e., weight) to a goal (the minimization of the expected likelihood).

\subsection{Adaptive DropConnect}
Learning can be accelerated by applying NaaA to the optimizer.
In this paper, the application of NaaA to SGD was named {\em Adaptive DropConnect} (ADC), 
the finalization of which can be interpreted as a combination of DropConnect \citep{wan2013regularization} and Adaptive DropOut \citep{ba2013adaptive}.
In the subsequent section, ADC is introduced as a potential NaaA application.

ADC uses NaaA for supervised optimization problems with multiple revisions.
In such problems, the first step is the presentation of an input state (such as an image) by the environment. Agents are expected to update their parameters to maximize the rewards presented by a criterion calculator.
Criterion calculators gives batch-likelihoods to agents, representing rewards.
Each agent, a classifier, updates its weights to maximize the reward from the criterion calculator.
These weights are recorded as an internal state.
A heuristic utilizing the absolute value of weight $|w_{ijt}|$ (the
technique used by Adaptive DropOut) was applied as the counterfactual return $o_{ijt}$.
The absolute value of weights was used because it represented the updated amounts for which the magnitude of error of unit outputs was proportional to $|w_{ijt}|$.

This algorithm is presented as Algorithm 2.
Because the algorithm is quite simple, it can be easily implemented and, thus, applied to most general deep learning problems such as 
image recognition, sound recognition, and even deep reinforcement learning.

\begin{algorithm}[t]
\caption{Adaptive DropConnect}
\begin{algorithmic}[1]
	\FOR{ $t=1$ \TO $T$ }
		\STATE Compute a bidding price for every edge: \textbf{for} $(v_j, v_i) \in \edges$ \textbf{do} \
		$b_{ijt} \leftarrow |w_{ijt}|$ 
		\STATE Compute an asking price for every node: \textbf{for} $\unit \in \units$ \textbf{do} \
		$\opt{q}_{it} \leftarrow \argmax_{q \in [0, \infty)} q d_{it}(q).$
		\FOR{$(v_i, v_j) \in \edges$}
				%\STATE Sample $u$ from a Bernoulli distribution: $u \sim \mathrm{Bernoulli}(\varepsilon)$
				%\STATE Sample $m$ from a Bernoulli distribution: $m \sim \mathrm{Bernoulli}(1/2)$
				\STATE Compute allocation: $g_{jit} \leftarrow H(b_{jit} - \opt{q}_{it})$ 
		\ENDFOR
		\STATE Sample a switching matrix $U_t$ from a Bernoulli distribution: $U_t \sim \mathrm{Bernoulli}(\varepsilon)$
		\STATE Sample the random mask $M_t$ from a Bernoulli distribution: $M_t \sim \mathrm{Bernoulli}(1/2)$
		\STATE Generate the adaptive mask: $M_t' \leftarrow U_t \circ M_t + (1 - U_t) \circ G_{ijt}$ 
		\STATE Compute $\vect{h}_t$ for making a shipment:
			$\vect{h}_t \leftarrow (M_t' \circ W_t) \vect{x}_t + \vect{b}_t$
		\STATE Update $W_t$ and $\vect{b}_t$ by backpropagation.
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Experiment}

\subsubsection{Setup}
This experiment verified the performance of two tasks: classification and single-agent reinforcement learning.

For classification, three types of datasets were used: MNIST, CIFAR-10, and STL-10. 
The given task was to predict the label of each image, and each dataset had a class number of 10.
The first dataset, MNIST, was a collection of black and white images of handwritten digits sized 28Å~28. The training and test sets contained 60,000 and 10,000 example images, respectively. 
The CIFAR-10 dataset images were colored and sized 32Å~32, and the assigned task was to predict what was shown in each picture. This dataset contained 6,000 images per class (5,000 for training and 1,000 for testing).
The STL-10 dataset was used for image recognition, and had 1,300 images for each class (500 training, 800 testing). Each image was sized 96Å~96; however, for the experiment, the images were resized to 48Å~48 because the greater resolution of this dataset (relative to the above datasets) required far more computing time and resources.

Next, the single-agent reinforcement learning task was set as 
the CartPole task from OpenAI gym with visual inputs.
In this setting, the agent was required to balance a pole while moving a cart.
The images contained a large amount of non-useful information, making pixel pruning important.

\subsubsection{Model}
Two models were compared in this experiment: DropConnect and Adaptive DropConnect (the model proposed in this paper). The baseline model was composed of two convolutional layers and two fully connected layers whose outputs are dropped out (we set the possibility as 0.5). The labels of input data were predicted using log-softmaxed values from the last fully connected layer. In the DropConnect and Adaptive DropConnect models, the first fully connected layer was replaced by a DropConnected and Adaptive DropConnected layer, respectively. It should be noted that the DropConnect model corresponded to the proposed method when $\varepsilon$ = 1.0, meaning agents did not perform their auctions but instead randomly masked their weights.

\subsubsection{Results}
The models were trained over ten epochs using the MNIST datasets, and were then evaluated using the test data. The CIFAR-10 and STL-10 epoch numbers were 20 and 40, respectively. Experiments were repeated 20 times for each condition, and the averages and standard deviations of error rates were  calculated. Results are shown in Table \ref{tbl:cls}. As expected, the Adaptive DropConnect model performed with a lower classification error rate than either the baseline or DropConnect models regardless of the given experimental datasets.


\begin{table}[h]
	\caption{ Experimental result for image classification tasks and single-agent RL }\label{tbl:cls}. 
\centering
\begin{tabular}{l|ccc|c}
\hline
		& MNIST & CIFAR-10 & STL-10 & CartPole \\
\hline
		DropConnect \citep{wan2013regularization}	&	1.72 $\pm$ 0.160	&	43.14 $\pm$ 1.335	&	50.92 $\pm$ 1.322 & 285 $\pm$ 21.5 \\
		Adaptive DropConnect	&	\textbf{1.36} $\pm$ 0.132	&	\textbf{39.84} $\pm$ 1.035	&	\textbf{42.17} $\pm$ 2.329 & \textbf{347} $\pm$ 29.4 \\
\hline
\end{tabular}
\end{table}

