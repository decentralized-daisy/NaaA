\section{Related Work}
現在成功している深層強化学習のモデルの多くは、単一のエージェントが環境の観測から認知、行動決定といった一連のプロセスを担う。
DQN \citep{mnih2015human,silver2016mastering} は、Atari のスクリーン系列から最適な行動を決定したり、AlphaGo のモジュールとして囲碁の盤面から勝利に最も近い一手を選ぶ。
DDPG \citep{lillicrap2015continuous} は物理空間において摩擦や重力係数などの条件を考慮した多関節の制御を実現する。
単一のエージェントを用いて強化学習を解くという試みは、人間の持つ身体性のアナロジーから考えると一見して妥当であるように思えるが、現実世界は open world であり、単一のエージェントが完全に情報が観測することが難しい。
そのためマルチエージェントによるアプローチが求められている。
%A3C \citep{mnih2016asynchronous} 

深層強化学習を POMDP 環境に適用する研究はいくつか行われている。
Deep Recurrent Q-Network (DRQN) \citep{sorokin2015deep} は、
隠れマルコフ連鎖を想定し、リカレントニューラルネットワーク(RNN)を用いて真の状態を推定している。
他にも、エレベーター制御\citep{crites1998elevator}、センサーネットワーク\citep{fox2000probabilistic}、ロボットサッカー\citep{stone1998towards}などがマルチエージェントによって解かれている（あとで修正: 専門家に確認中）。

マルチエージェントシステムによる強化学習へのアプローチにはいくつかの方法がある。
一つは、学習の効率を高めるために、同一のモデルに従う複数のエージェントを用いて探索を行う方法であり、Gorilla \citep{nair2015massively} , A3C \citep{mnih2016asynchronous} などで採用されている。
二つ目は、アクチュエーターを増やすことによって行動の量を増やす方法であり、サッカーゲーム \citep{kalyanakrishnan2006half} やテレビゲーム \citep{tampuu2017multiagent} で行われている。
三つ目は、センサーを増やすことによって観測の量を増やす方法であり、自動運転\citep{sukhbaatar2016learning}やセンサーネットワーク\citep{fox2000probabilistic}などで行われている。
本研究は、観測困難な環境からいかに良い状態表現を得るかに注目しているため、三つ目のケースをスコープとする。

本研究に近い研究は CommNet \citep{sukhbaatar2016learning} である。
CommNet は、マルチエージェントシステム全体を一つのニューラルネットワークとみなし、
バックプロパゲーションによって学習を行っている。
彼らの手法は、すべてのエージェントが協力的であるという仮定を置いており、すべてのエージェントが一つの目的関数を最大化する。
本研究では、エージェントが協力的でなく利己的、すなわち裏切り行動をとる可能性がある場合を対象にしている点が異なる。また、中央集権的に誤差情報を管理して分配するサーバが必要なく、完全分散環境で動作することも特徴である。
実際、IoT のような実環境における問題を考えると、センサーを持っている主体は異なる人物であるために、必ず協力行動をとるとは考えにくい。
本研究はこのような環境においても動作する。

マルチエージェントで強化学習の問題を解決する場合には、信頼度割り当て問題の解決が重要になる。
そこで、エージェントの信頼度を、そのエージェントがいた場合と、
いなかったと仮定した場合の差として定量化する研究が行われている。
QUICR-learning \citep{agogino2006quicr} では、エージェント $i$ が reward $R(a_t)$ の代わりに、
そのエージェントがある行動 $a_{it}$ をとった場合 $a_t$ と取らなかった場合 $a_t-a_{ti}$ の差、
counterfractual reward $R(a_t) - R(a_t - a_{it})$ の cummulative discount summation を最大化している。
COMA \citep{foerster2017counterfactual} は、actor-critic において critic が共通しており、actor がマルチエージェントであるという actor-critic の仕組みを考え、それぞれの actor が counterfractual reward を最大化するような仕組みを考えている。

マルチエージェントによる観測および認知の枠組みは、センサー処理の分野ではエッジコンピューティング\citep{bonomi2012fog}としても知られている。
エッジコンピューティングは分散環境を前提とした信号処理のモデルであり、一つの処理系がすべてのデータを処理するのではなく、複数のセンサーの情報を一つのエッジサーバが集約し、複数のエッジサーバが次元削減したデータをデータセンターに送るという階層的な構造をしている。
本研究は、実際に IoT の環境で本研究が適用されるケースを想定している。
報酬はビットコインなどの決済手段によって送金が可能であるため、
Web 全体でスケーラブルなモデルが実現できる可能性がある。
