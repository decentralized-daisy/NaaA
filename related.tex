\section{Related Work}
%深層強化学習は emerging topic であり、DQN を皮切りに多くの研究が行われている。
%DQN では、
深層強化学習を POMDP 環境に適用する場合、観測困難な環境からいかに真の状態を推定するかが重要になる。
Deep Recurrent Q-Network (DRQN) \citep{sorokin2015deep} は、
隠れマルコフ連鎖を想定し、リカレントニューラルネットワーク(RNN)を用いて真の状態を推定している。

マルチエージェントで強化学習の問題を解決する場合には、信頼度割り当て問題の解決が重要になる。
そこで、エージェントの信頼度を、そのエージェントがいた場合と、
いなかったと仮定した場合の差として定量化する研究が行われている。
QUICR-learning \citep{agogino2006quicr} では、エージェント $i$ が reward $R(a_t)$ の代わりに、
そのエージェントがある行動 $a_{ti}$ をとった場合 $a_t$ と取らなかった場合 $a_t-a_{ti}$ の差、
counterfractual reward $R(a_t) - R(a_t - a_{ti})$ の減衰和を最大化している。
COMA \citep{foerster2017counterfactual} は、actor-critic において critic が共通しており、actor がマルチエージェントであるという actor-critic の仕組みを考え、それぞれの actor が counterfractual reward を最大化するような仕組みを考えている。

これらの研究の問題は、情報がすべて共有されているという前提に立っており、
信頼度を割り当てるという性善説に基づいている点にある。
そのため、裏切ることが考えられる人物がいると、予想外の内容が学習されてしまう。
たとえば、IoT のような実環境における問題を考えると、センサーを持っている主体は異なる人物であるために、
協力行動をとるとは考えにくい。

本研究では、すべてのニューロンをエージェントとみなす方法を提案している。
%これらのアイデアを、すべてのニューロンをエージェントとみなすところに拡張している。

%counterfractual reward を用いるというアイデアは \cite{} に基づいている。
%本研究の新規性は、メカニズムデザインを用いて情報の価値を評価する仕組みを作った点である。
%マルチエージェントにおける通信の課題について研究を行っている文献もある。
%ジレンマ問題を扱っている。
%一方で、環境を推定するという方法も取られている。
%これらに対して本研究では、

\if0
（作成中）以下の分野について言及

強化学習(DQN, DRQN, DARQN)

マルチエージェントシステム(QUICR, ICLRサッカー)

マルチエージェント強化学習

ジレンマ問題を解決する研究
\fi
