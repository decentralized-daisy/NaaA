\section{Background}
% Ç±Ç±Ç≈åæÇ¢ÇΩÇ¢Ç±Ç∆ÅFâÇ´ÇΩÇ¢ñ‚ëËÇÕâΩÇ»ÇÃÇ© (POSG Ç∆Ç¢Ç§ì¡éÍÇ»óÃàÊÇ…Ç¬Ç¢Çƒê‡ñæÇ∑ÇÈÅBMDP Ç∆ÇÕÇ«Ç§à·Ç§Ç©Åj

First, we consider a POMDP environment in which a single agent acts.
The POMDP environment is a seven-tuple $(\S, \A, \Trans, \R, \Observations, \ObProb, \gamma)$,
where $\S$ represents a set of states, $\A$ stands for a set of actions, $\Trans$ denotes a transitive probability, 
$\R: \S \times \A \rightarrow \Real $ is a function from the state and the action of an agent to the real value.
$\Observations$ represents a possible set of observations, $\ObProb$ denotes a set of observation probability, and
$\gamma$ is the discount rate.
An agent partially predicts state $h \in \S$ through an observation $s \in \Observations$.

Generally, $s$ has higher dimensions than $h$, and is complex.
For example, although Atari 2600 has a read only memory (RAM) as the true state, which contains 128 bytes,
the generated image from that $s$ has more than 10,000 dimensions.
Therefore, DQN and DRQN abstract $s$, and create original state representation to predict good action efficiently.
(Although the original paper of DQN assumes MDP, the paper of DRQN pointed out that the environment is POMDP).
Although DQN does not address the state transition directly because it is model-free method, 
some interpretations hold that the hidden state representation is learned in the previous layer of the output layer \citep{zahavy2016graying}
Using the method below, we assume that the agent chooses an action through a neural network.
The POSG environment is a natural extension of POMDP to multi-agent environment defined by a tuple $(\S, \A^i, \Trans, \R^i, \Observations^i, \ObProb^i, \gamma^i)_{i \in \mathcal{I}}$,
where $\mathcal{I}$ is a finite set of agents indexed 1, \dots, $N$.
Each agent has a policy $\pi_i: \Observations^i \rightarrow \A^i$. 
They maximize their return by interacting with the environment.

%The POSG environment is a multi-agent environment defined by a tuple $(\S, \A^i, \Trans, \R^i, \Observations^i, \ObProb^i, \gamma^i)_{i \in \mathcal{I}}$,
%where $\mathcal{I}$ is a finite set of agents indexed 1, \dots, $N$, 
%$\S$ represents a set of states, 
%$\A^i$ stands for a set of actions, 
%$\Trans$ denotes a transitive probability, and 
%$\R^i: \S \times \A^1 \times \cdots \times \A^N \rightarrow \Real $ is a function from the state and the action of all agents to the real value.
%$\Observations^i$ represents a possible set of observations. $\ObProb^i$ denotes a set of observation probability.
%$\gamma^i$ is the discount rate.
%Each agent has a policy $\pi_i: \Observations^i \rightarrow \A^i$. They maximize their return by interacting with the environment.
%A difference from POMDP is that there are $N$-agents.

We employ several concepts from game theory.
Although RL and game theory are typically investigated in parallel, several concepts in game theory can be written in the domain of RL.
The (Bayesian) Nash equilibrium $\hat{\pi}_{i}$ is a policy by which all agents maximize their expected reward. That is,
\begin{flalign}
\hat{\pi}(s_{it}) = \argmax_{a_i \in \A^i} \Expect{ \vect{a}_{-i} \in A^{-i}, h \sim \ObProb^i }{ \R^i(h, \vect{a}) | s_{it}} \, \forall i \in \mathcal{I},
\end{flalign}
where $\mathcal{A}^{-i}$ is a set of actions except for $i$. 
Intuitively, the equation took an expected value of reward to integrate out other agents' unobserved actions.
Because the Nash equilibrium is sufficient to state only the best action in the most cases, we use the notation with action $\hat{\vect{a}}$ in the following.

