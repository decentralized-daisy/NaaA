% What のみを書く - Why はイントロへ
\section{Background}
まず、単一のエージェントが PODMP 環境で行動する場合について考える。
POMDP 環境とは 7 つ組 $(\S, \A, \Trans, \R, \Observations, \ObProb, \gamma)$ である。
ただし、$\S$は状態集合、$\A$ は行動集合、$\Trans$ は遷移確率、$\Observations$ は取りうる観測の集合、$\ObProb$ は観測の集合、$\gamma$ は減衰率である。
エージェントは観測 $o \in \Observations$ を通して部分的に状態 $\S$ を推定する。
一般に $o$ の方が $s$ よりも次元が大きく、複雑である。
たとえば、Atari 2600 は真の状態である RAM は 128 バイトしかないが、そこから生成される画像 $o$ は 10,000 以上の次元を持っている。
そのため、DQN や DRQN では、$o$ の情報を抽象化し、独自の状態表現を作っていると解釈できる（DQN の原著論文では MDP であることを前提としているが、DRQN の論文で環境が POMDP であることが主張されている）。
もちろん、DQN はモデルフリーの手法であるため、直接は状態遷移を扱わないが、出力層の一個手前の層に状態が格納されているという解釈もできる\citep{zahavy2016graying}。
以下では、エージェントはニューラルネットワークを通して行動を決定することを前提とする。

次に、互いに通信するマルチエージェントシステムを考える。
一般にエージェントが多いほど、観測を増やすことが可能である。
たとえば、自動運転のケースでは自動車同士が通信することでより正確な世界に対する知識を得ることができる。
この時、エージェントが持っているニューラルネットワーク同士をつなげる方法がとられている\citep{sukhbaatar2016learning}。
これは、マルチエージェントシステム全体を一つのニューラルネットワークをとらえることができると考えられる。
そこで、本研究ではこれを拡張し、すべてのユニットをエージェントとみなす。
%すなわち、ニューラルネットワークにおける各ユニットをエージェントとみなした場合の、最適化を行う。

%以下では、ニューラルネットワークとは広義の
%DRQN では隠れた状態の推定はニューラルネットワークを使って行われることが多い。
%リバースエンジニアリングしていると解釈できる。


