\documentclass{article} % For LaTeX2e
\usepackage{iclr2017_conference,times}
\usepackage{url}
\usepackage{amsthm}
\input{preamble.tex}


\title{Neuron as an Agent}

%\author{Shohei Ohsawa, Kei Akuzawa, Yusuke Iwasawa \& Yutaka Matsuo \\
%The University of Tokyo\\
%7 Chome-3-1 Hongo, Bunkyo, Tokyo \\
%\texttt{ohsawa@weblab.t.u-tokyo.ac.jp} \\
%}
\author{Anonymous}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
We propose {\em Neuron as an Agent} (NaaA) as a novel framework for reinforcement learning (RL), and show its optimizing mehod.
NaaA considers all the units in a neural network as agents, and optimizes the reward distribution as a multi-agent RL problem.
Firstly, with showing the optimization of NaaA, we report the negative result that the performance decreases if we naively consider the units as agents.
As solution of the problem, we introduce a mechanism from game theory.
As theoretical result, we show that the agent obeys to maximize its {\em counterfactual return} as the Nash equilibrium of the mechanism.
After that, we show that learning counterfactural return leads the model to learning optimal topology between the units,
and propose {\em adaptive dropconnect}, a natural extension of dropconnect.
At the last, we confirm that optimization with the framework of NaaA leads better performance of RL, with numerical experiments.
Specifically, we use a single-agent environment from Open AI gym, and multi-agent environment from ViZDoom.
\end{abstract}

\input{introduction3-en.tex}
\input{related-en.tex}
\input{background-en.tex}
\input{method-en.tex}
\input{optimization-en.tex}
\input{experiment.tex}
\input{discussion-en.tex}
\input{conclusion-en.tex}
\input{appendix.tex}

\bibliography{daisy}
\bibliographystyle{iclr2017_conference}

\end{document}

