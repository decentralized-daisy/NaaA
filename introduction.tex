\section{Introduction}
% ・多分一回 MTG した方がいい（ストーリーの作り方、トピックセンテンス）
% ・有用な観測に対してインセンティブを与える
% ・高次のニューロンも、見方によっては下位のニューロンの信号を観測するメタセンサーであると考えられる 
% このあたりはまだストーリー作りできていません。

マルチエージェントによる強化学習が現実の課題に対して有効である理由は、興味深いことに表現学習の原理と一致する：有用な表現は予測性能を向上させる。
現実にある問題の多くは、DQN が前提としているマルコフ決定過程(MDP)ではなく部分的観測マルコフ決定過程(POMDP)である\citep[p.258]{sutton1998reinforcement}。
POMDP において真の状態は完全には不可視であり、良質な観測結果が良質な行動に結びつくため、有限のリソースを使って何を観測するかの設計が必要である。
必然的に、観測に用いられるエージェントが多いほど、将来的に獲得できる報酬も高くなる。

本研究に近い研究は CommNet \citep{sukhbaatar2016learning} である。
CommNet は、マルチエージェントシステム全体を一つのニューラルネットワークとみなし、
バックプロパゲーションによって学習を行っている。
彼らの手法は、すべてのエージェントが協力的であるという仮定を置いており、すべてのエージェントが一つの目的関数を最大化する。
本研究では、エージェントが協力的でなく利己的、すなわち裏切り行動をとる可能性がある場合を対象にしている点が異なる。また、中央集権的に誤差情報を管理して分配するサーバが必要なく、完全分散環境で動作することも特徴である。
実際、IoT のような実環境における問題を考えると、センサーを持っている主体は異なる人物であるために、必ず協力行動をとるとは考えにくい。
本研究はこのような環境においても動作する。

マルチエージェント学習において、エージェント間の通信方法の最適化が重要である。
マルチエージェントの設定では、エージェントをユニットとみなし、end-to-end での最適化を行う事例が報告されている。
CommNet \citep{sukhbaatar2016learning} では、エージェント間でのコミュニケーションにバックプロパゲーションを用いている。
エージェントは、系全体の利益を最大化するような情報を出力するよう訓練される。
エージェントがユニットとみなせるのであれば、その逆は成立するのだろうか？
すなわち、ユニット単体を、自己のリターンを最大化するエージェントとみなすことは可能だろうか？
本論文は、以下のことを主張する。

%従来研究でなされていないことは、エージェントが利己的に動作すると仮定した場合の最適化であるが、
%これは容易には実現しない。
%そのために、ユニットが自己の報酬を最大化するという場合において

\begin{center}
環境の観測について、エージェントとユニットの役割は等価である。
\end{center}
%また、複数のモデルを組み合わせることによって精度が高まるアンサンブル法としての側面が存在する。
%有用な状態表現が精度の向上に寄与するためである。

%======OK=========


本研究のゴールは、すべてのユニットが自律的に動作すると仮定した場合に、システム全体が獲得する累積報酬（リターン）を最大化することである。
これを、新しいタスクである {\em Neuron as an Agent} (NaaA) として定式化する。
NaaA は、パーセプトロンや Convolutional Net (CNN) などの微分可能(differentiable)な関数で構成されるニューラルネットワークの個々のユニットを、自身のリターンを利己的に最大化する、非協力的なエージェントであると仮定する。
通常、ニューラルネットワークの目的は、目標値と予測値の間の誤差などの、共通の criterion $e$ の最小化である。
そのため、各ユニットはバックプロパゲーションによって出力 $y$ による微分 $\partial e / \partial y$ を計算し、$e$ が小さくなる方向に $y$ の大きさを制御していた。
NaaA のユニットは、このように共通の値を最適化するということを陽に目的としない。
そのため、CommNet \citep{sukhbaatar2016learning} のようなマルチエージェントシステムのように中央集権的なコントローラを持たず、
環境に存在する複数のエージェントが自律的に動作するという特徴を持つ。
したがって NaaA はスケーラブルであり、解くことが困難であるとされている open world である現実世界の問題（e.g., 自動運転、IoT、株価予測）を解くために、任意の設計者が仕組みに参入することを許容している。

一般に、ジレンマの存在により、マルチエージェントシステムとしてニューラルネットワークを構築することは容易ではない。
ジレンマとは、個別最適であるナッシュ均衡が全体最適であるパレート効率性を達成しない問題である。
本研究では、報酬の分配にゲーム理論におけるメカニズムの一つである digital goods auction \citep{guruswami2005profit} を用いることで、自然にパレート効率性が達成されることを示す。
ナッシュ均衡として、ユニットは予測誤差の最小化の代わりに、そのユニットが存在する場合と存在しない場合の系全体のリターンの差である counterfactual return を最大化する。
これは、マルチエージェントシステムの報酬分配問題に対して提案されている指標である counterfactual reward \citep{agogino2006quicr} を時間方向に拡張したものである。
%本研究ではジレンマ問題を扱うために、ゲーム理論の一つであるメカニズムデザインを用いることで、
%なぜなら、複数のエージェントが自己の報酬を追求することは、系全体が獲得する報酬を最大化を保証するとは限らないからである。

NaaA のユニットは、counterfactual return の期待値を予測するための
{\em valuation net} を持つ。
すなわち、系全体は各ユニットがニューラルネットワークを持つ入れ子構造をしている。
本論文では、valuation net の構成例や、強化学習を通して訓練する方法を述べている。

%さらに、intrinsic value の予測を行う、Valuatio、報酬がユニットの本源的価値(intrinsic value)を反映して配分されることを示す。n Net を提案する。
%ここでいう本源的価値とは、ユニットが存在する場合と存在しない場合の差である counterfactual reward \citep{agogino2006quicr} の累積減衰和の期待値(expectation of cummulative discount value)のことである。
%オークション理論を用いてジレンマ問題を解決した上で、
%以下では報酬分配のフレームワークである profit maximization について述べ、

実験では、標準的な強化学習のタスクによる数値実験を用いて NaaA が POMDP の問題の精度を高めることを示す。
具体的に、Atari および VisDoom における環境を用いて、既存研究が DQN や A3C を上回ることを示す。
