\section{Introduction}
% ・多分一回 MTG した方がいい（ストーリーの作り方、トピックセンテンス）
% ・有用な観測に対してインセンティブを与える
% ・高次のニューロンも、見方によっては下位のニューロンの信号を観測するメタセンサーであると考えられる 
% このあたりはまだストーリー作りできていません。

マルチエージェントによる強化学習が現実の課題に対して有効である理由は、興味深いことに表現学習の原理と一致する：有用な表現は予測性能を向上させる。
現実にある問題の多くは、DQN が前提としているマルコフ決定過程(MDP)ではなく部分的観測マルコフ決定過程(POMDP)である\citep[p.258]{sutton1998reinforcement}。
POMDP において真の状態は完全には不可視であり、良質な観測結果が良質な行動に結びつくため、有限のリソースを使って何を観測するかの設計が必要である。
必然的に、観測に用いられるエージェントが多いほど、将来的に獲得できる報酬も高くなる。
本論文は、以下のことを主張する。

\begin{center}
環境の観測について、エージェントとユニットの役割は等価である。
\end{center}
%また、複数のモデルを組み合わせることによって精度が高まるアンサンブル法としての側面が存在する。
%有用な状態表現が精度の向上に寄与するためである。

%======OK=========

本研究のゴールは、すべてのユニットが自律的に動作すると仮定した場合に、システム全体が獲得する累積報酬（リターン）を最大化することである。
これを、新しいタスクである {\em Neuron as an Agent} (NaaA) として定式化する。
NaaA は微分可能(differentiable)な関数で構成されるニューラルネットワークの個々のユニットを、自身のリターンを利己的に最大化する、非協力的なエージェントであると仮定する。
通常、ニューラルネットワークの目的は、目標値と予測値の間の誤差などの、共通の criteiron $e$ の最小化である。
そのため、各ユニットはバックプロパゲーションによって出力 $y$ による微分 $\partial e / \partial y$ を計算し、$e$ が小さくなる方向に $y$ の大きさを制御していた。
NaaA のユニットは、このように共通の値を最適化するということを陽に目的としない。
そのため、CommNet \citep{sukhbaatar2016learning} のようなマルチエージェントシステムのように中央集権的なコントローラを持たず、
環境に存在する複数のエージェントが自律的に動作するという特徴を持つ。
したがって NaaA はスケーラブルであり、解くことが困難であるとされている open world である現実世界の問題（e.g., 自動運転、IoT、株価予測）を解くために、任意の設計者が仕組みに参入することを許容している。

一般に、ジレンマの存在により、マルチエージェントシステムとしてニューラルネットワークを構築することは容易ではない。
%なぜなら、複数のエージェントが自己の報酬を追求することは、系全体が獲得する報酬を最大化を保証するとは限らないからである。
ジレンマとは、個別最適であるナッシュ均衡が全体最適であるパレート効率性を達成しない問題である。
%本研究ではジレンマ問題を扱うために、ゲーム理論の一つであるメカニズムデザインを用いることで、
本研究では、報酬の分配にゲーム理論におけるメカニズムの一つである digital goods auction \citep{guruswami2005profit} を用いることで、自然にパレート効率性が達成されることを示す。
ナッシュ均衡として、ユニットは予測誤差の最小化の代わりに、そのユニットが存在する場合と存在しない場合の系全体のリターンの差である counterfactual return を最大化する。
これは、マルチエージェントシステムの報酬分配問題に対して提案されている指標である counterfactual reward \citep{agogino2006quicr} を時間方向に拡張したものである。

NaaA のユニットは、counterfactural return の期待値を予測するための
{\em valuation net} を持つ。
すなわち、系全体は各ユニットがニューラルネットワークを持つ入れ子構造をしている。
本論文では、valuation net の構成例や、強化学習を通して訓練する方法を述べている。

%さらに、intrinsic value の予測を行う、Valuatio、報酬がユニットの本源的価値(intrinsic value)を反映して配分されることを示す。n Net を提案する。
%ここでいう本源的価値とは、ユニットが存在する場合と存在しない場合の差である counterfactual reward \citep{agogino2006quicr} の累積減衰和の期待値(expectation of cummulative discount value)のことである。
%オークション理論を用いてジレンマ問題を解決した上で、
%以下では報酬分配のフレームワークである profit maximization について述べ、

実験では、標準的な強化学習のタスクによる数値実験を用いて NaaA が POMDP の問題の精度を高めることを示す。
具体的に、Atari および VisDoom における環境を用いて、既存研究が DQN や A3C を上回ることを示す。
