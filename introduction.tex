\section{Introduction}

% ・有用な観測に対してインセンティブを与える
% ・高次のニューロンも、見方によっては下位のニューロンの信号を観測するメタセンサーであると考えられる 

現在成功している深層強化学習のモデルの多くは、単一のエージェントが環境の観測から認知、行動決定といった一連のプロセスを担う。
DQN \citep{mnih2015human} や A3C \citep{mnih2016asynchronous} は、Atari のスクリーン系列から最適な行動を決定する。
AlphaGo \citep{silver2016mastering} は囲碁の盤面から勝利に最も近い一手を選ぶ。
DDPG \citep{lillicrap2015continuous} は物理空間において、最適な筋肉の動きを決定する生物の動きを学習する。
単一のエージェントを用いて強化学習を解くという試みは、人間の持つ身体性のアナロジーから考えると一見して妥当であるように思える。

しかし、現実世界の問題の多くはマルチエージェントによって解決される。
すなわち、観測、認知、行動決定がすべて異なるエージェントによって行われる。
各エージェントがセンシングした状態空間は、エージェント同士のコミュニケーションを通して統合されていき、最終的に報酬系との接点を持つアクチュエーターの行動に用いられる。
%エージェントに報酬を配分する様子として観察される。
たとえば、自動運転車は、自身だけでなく、周辺の車から得られた情報を統合することで衝突を減らすことができる。
また、株のトレーダーは、多くの人物から提供された情報をもとに推論し、有用な情報提供者には対価を支払う。
他にも、エレベーター制御\citep{crites1998elevator}、センサーネットワーク\citep{fox2000probabilistic}、ロボットサッカー\citep{stone1998towards}などがマルチエージェントによって解かれている。

マルチエージェントによる問題解決が現実の課題に対して有効である理由は、興味深いことに表現学習の原理と一致する：有用な表現は機械学習の性能を向上させる。%有用な状態表現が精度の向上に寄与するためである。
現実にある問題の多くは、DQN や A3C が前提としているマルコフ決定過程(MDP)ではなく部分的観測マルコフ決定過程(POMDP)である\citep[p.258]{sutton1998reinforcement}。
POMDP において真の状態は完全には不可視であり、良質な観測結果が良質な行動に結びつくため、アテンション、すなわち、有限のリソースを使って何を観測するかの設計が必要である。
したがって、観測に用いられるエージェントが多いほど、将来的に獲得できる報酬も高くなる。
また、複数のモデルを組み合わせることによって精度が高まるアンサンブル法としての側面が存在する。

本論文は、表現学習の群強化学習への自然な拡張として、
ディープニューラルネットワークをマルチエージェントシステムとして考える。
マルチエージェントによる観測および認知の枠組みは、センサー処理の分野ではエッジコンピューティング\citep{bonomi2012fog}としても知られている。
エッジコンピューティングは分散環境を前提とした信号処理のモデルであり、一つの処理系がすべてのデータを処理するのではなく、複数のセンサーの情報を一つのエッジサーバが集約し、複数のエッジサーバが次元削減したデータをデータセンターに送るという階層的な構造をしている。
実際、人間の脳を観察しても、各神経細胞は独立して動作する。
神経細胞は胚細胞からの発達段階において Nerve Growth Factor、BDNF といった神経栄養因子(NTF)を追求することが知られており、十分な NTF を受け取ることができなかったニューロンはアポトーシスを引き起こして自死することが知られている\citep{almeida2005neuroprotection}。
こうした観察から、各神経細胞を独立した生物として捉える見方はニューラルダーウィズム\citep{edelman1987neural}と呼ばれる。

%======OK=========

% 問題
マルチエージェントによる問題解決では、信頼度割り当て問題(credit assignment problem)が発生する。
既存研究として、複数のエージェントを想定したものがあるが\citep{agogino2006quicr}、
エージェント間が直列に情報を伝達し合う状況における報酬分配については解が与えられていない。
本論文で提案する学習の枠組み Neuron as an Agent (NaaA) は、ゲーム理論を想定し、
エージェント間で報酬の分配を行う「通貨」を想定することで信頼度割り当ての問題を扱う。
%ゲーム理論の一つであるメカニズムデザインの考え方を用いて、信頼度の最適配分を実現する。
利益は報酬とコストの差分であり、ニューロンは下位のレイヤーから情報を買い、上位のレイヤーに対して信号を"売る"ことで報酬を得る。利益は、ニューロン自体が生成した付加価値であり、売価と買価を差し引いて計算される。

ゲーム理論を用いて機械学習の最適化を行う手法はいくつか提案されている\citep{stone1998towards,goodfellow2014generative}が、ゲームの設計方法によっては、ナッシュ均衡がパレート最適と一致しないというジレンマ\citep{holt2007markets}が発生する。
ナッシュ均衡とは個々のエージェントによる報酬追及の結果として得られる帰結であり、パレート最適とは設計者意図する全体最適な帰結である。
具体的に、個々のユニットが自身のコストを最小化した結果、入力ユニットに十分な報酬がいきわたらなくなる。

NaaAでは、経済学におけるメカニズムデザインを用いて、分散環境における信頼度割り当ての問題を解く。
メカニズムデザインでは、パレート最適な結果がナッシュ均衡の解と一致するようにする対戦略性の高い仕組みである。
NaaA は、ユニットが複数財オークションを実行することによって最適な価格を決定する仕組みである。
本論文では、メカニズムデザインの一つであるこの仕組みがエージェント間の社会的ジレンマを解決し、各ユニットが、他のユニットの本源的価値に基づく最適な価格設定を行うことを示す。
具体的に、同じレイヤーに属するニューロン同士を競合学習させることで、neural agent が他のニューロンの価値を正直に申告することを示す。
この報酬モデルは、エージェントがどの情報に注目すべきかというヒントを与える。
すなわち、これはアテンションモデル\citep{xu2015show}を拡張であると解釈できる。

実験では、標準的な強化学習のタスクによる数値実験を用いて NaaA が POMDP の問題の精度を高めることを示す。
具体的に、Atari および VisDoom における環境を用いて、既存研究が DQN や A3C を上回ることを示す。
%また、全体として消費エネルギーが下がることも示す。
%これが二つ目の貢献である。


%本研究では、Neuron as an Agent を提案する。
%NaaA は、





%%==================================
%現在成功している深層強化学習のモデルの多くは、
%単一のエージェントが環境の観測から認知、行動決定といった一連のプロセスを担う。
%
%一方で、現実世界の問題を解決するためのマルチエージェントによる手法が注目されている。
%
%マルチエージェントによる問題解決が現実の課題に対して有効である理由は、有用な状態表現が精度の向上に寄与するためである。
%Keyword: POMDP
%
%本論文は、表現学習の POMDP への自然な拡張として、
%ディープニューラルネットワークをマルチエージェントシステムとして考える。
%実際、人間の脳を観察しても、各神経細胞は独立して動作する。
%
%マルチエージェントによる問題解決では、信頼度割り当ての問題が発生する。
%・センサー配置問題
%
%本論文で提案する学習の枠組み Neuron as an Agent (NaaA) は、
%ゲーム理論の一つであるメカニズムデザインの考え方を用いて、信頼度の最適配分を実現する。
%
%実験では、標準的な強化学習のタスクによる数値実験を用いて NaaA が POMDP の問題の精度を高めることを示す。















%ニューラルネットワークはマルチエージェントシステムであると考えられる。
%下のレイヤーを状態と考えると、


%こうしたマルチエージェントの状況において、DQN や A3C などの単一エージェントを前提としたモデルはうまく動作しない。

%実際、ゲーム理論を用いて機械学習の最適化を行う手法はいくつか提案されている\citep{stone1998towards,goodfellow2014generative}が、ゲームの設計方法によっては、ナッシュ均衡がパレート最適と一致しないという社会的ジレンマ\citep{holt2007markets}が発生する。


%%では、これらの活動がすべて異なる主体によって達成される場合がありうる。
%%・近年の深層強化学習の成功は、強化学習に深層学習を組み合わせることの有用性を示している。
%
%% 問題は何か
%現在主流となっている深層強化学習の枠組みでは、
%単一のエージェントが問題空間の観測、認知、行動決定のすべてを担う。
%
%%しかし、現実世界の問題では、観測、認知、行動決定がすべて異なる主体によって達成される場合がありうる。
%%分散化する知能は、近年様々な分野で言われている。
%%ビットコインは、多くの通貨する仕組みである。
%
%このように、重要である。
%実際、人間の脳は分散化されたニューロンによって構成される。
%この考え方はニューラルダーウィズムと呼ばれる。
%ニューラルダーウィズムの提唱者である***は、彼の著書の中でこのように語っている。
%
%本論文はニューロンを分散化した主体としてみなす。
%
%一つのニューロンを分散化することは容易ではない。
%なぜなら、ニューロンは報酬
%
%NaaA は、
%
%ここで見方を変えて、脳の中でどのようにニューロンの間の情報供給がなされているか見てみよう。
%ニューロンによる報酬系のやり取りは脳の中でも行われている。
%ニューロンには血管からグリア細胞を通して
%すなわち、有用な情報を提供しないニューロンは自死する仕組みが脳内にプログラミングされている。
%
%入力側のエージェントにとって、情報処理に見合う対価を得る必要がある。
%これまで成功してきた深層学習モデル（DQNやA3C）など、
%単一のエージェントモデルは、これらの問いを扱うことができない。
%
%なぜなら、センサー、モデル、アクチュエータのすべてが平等に報酬を受け取る必要がある。
%これは、信用割り当て問題と呼ばれる。
%
%現実の世界に適用する場合、よりスケーラブルに考える場合、
%分散型環境に適用したい。
%
%一方、
%
%% 考え方
%本論文では、
%エージェント同氏が通貨を使ってやり取りする。
%ゲーム理論に基づき、エージェントが最適な価値を評価できるようにする。
%これらは、実世界においてもビットコインなどの仮想通貨における送金が可能であるモデルである。
%
%% 実際のオークションの仕組み
%通貨の価値を最大化するため、NaaA はオークションの理論を用いる。
%
%% 
%本論文の貢献は以下の通りである。
%・信頼度割当問題を、個々のニューロンをエージェントとみなす手法を提案することで解決した
%・提案手法の有用性を、数値実験を用いて確認した。
%
%NaaA が提案する報酬系の枠組みは、
%近年のビットコインなどによる報酬系とも組み合わせることができる。
%そのため、社会実装を視野に入れたモデルである。

%すべてのユニットをエージェントとみなす方法は、マルチエージェント協力ゲームとして最適化を行う。

% 何の問題を解決するか
% ここをもう少し強化: 問題が何なのか説明され切れていない
% エネルギーに関する話も書く
%こうしたマルチエージェントの状況において、DQN や A3C などの単一エージェントを前提としたモデルはうまく動作しない。
%その理由の一つに最適な報酬分配の問題がある。
%どのように報酬を分配するのかという、マルチエージェント学習における信頼度割り当ての問題に帰着される。

%経済モデルを用いて各ユニットをエージェントと考えるのは容易ではない。

%本論文で提案する学習の枠組み Neuron as an Agent (NaaA) は、
%経済とニューラルネットワークの理論の統合を試みる。
%既存研究におけるマルチエージェント学習を一般化し、
%ニューラルネットワークに含まれるユニットを自身の利益追求を行うエージェントとみなされる。
%NaaA では、中央集権的な主体は存在せず、ユニットはあくまで自身の利益を最大化する。
%NaaA のフレームワークでは、通貨を想定し、得られた報酬を分配することで信頼度割り当て問題を解く。
%ユニットが獲得する報酬関数は、利益、すなわち、報酬（上位のユニットから受け取った通貨）と、コスト（下位のユニットに支払った通貨）の差分として表現される。
%したがって、ユニットは

%以上が一つ目の貢献である。
%バックプロパゲーションの代わりにマルチエージェントシミュレーションによる最適化を行う。
%NaaA はまず、ニューロンの間で取引される通貨を用意し、ニューロン同士でやり取りするようにする。
%NaaA では、ゲーム理論を用いて複数のニューロンを競合させることで、信頼度割り当て問題を解決し、最適な報酬分配を実現する。

%ニューラルダーウィズムの提唱者であるエーデルマンは次のように語っている。
%\begin{quote}
%{\em
%... To survive in its econiche,
%an organism must either inherit or create criteria that
%enable it to partition theworld into perceptual categories
%according to its adaptive needs. Even after that
%partition occurs as a result of experience, the world
%remains to some extent an unlabeled place full of novelty.
%}
%\end{quote}

% TODO: まずは「伝える」ことを優先。修飾は後。
% TODO: Bengio が Neural Darwinism に対してなんといっているかを書く

%・現在主流となっている深層強化学習のモデルの多くは、単一のエージェントが問題空間の観測、認知、行動決定のすべてのプロセスを担う。
%	・DQN や A3C は、Atari のスクリーン系列から最適な行動を決定する。
%	・AlphaGo は囲碁の盤面から勝利に最も近い一手を選ぶ。
%	・DDPG は物理空間において、最適な筋肉の動きを決定する生物の動きを学習する。
%	・単一のエージェントを用いて強化学習を解くという試みは、人間の持つ身体性のアナロジーから考えると一件妥当であるように思える。
%
%・しかし、こうした既存研究の成功にかかわらず、現実世界の問題はマルチエージェントである。
%	・すなわち、観測、認知、行動決定がすべて異なる主体によって行われる。
%	・実際、人間の脳を観察しても、各神経細胞は独立した生物ととらえることが可能である。こうした見方はニューラルダーウィズムとも呼ばれる。
%
%・こうしたマルチエージェントの状況において、従来手法はうまく動作しない。
%	・その理由の一つに最適な報酬分配の問題が存在する。
%	・どのように報酬を分配するのかという、マルチエージェント学習における信頼度割り当ての問題に帰着される。
%	・いくつかの文献は協力ゲームを扱っているが、エージェント間が直列に情報を伝達し合う状況における報酬分配については解が与えられていない。
%
%・本論文では、マルチエージェント学習に特化した枠組み、Neuron as an Agent (NaaA) を提案する。
%	・NaaA では、ゲーム理論を用いて複数のニューロンを競合させることで、信頼度割り当て問題を解決し、最適な報酬分配を実現する。
%	・これはアテンションモデルを拡張したモデルとして解釈できる。
%	・これが一つ目の貢献である。
%
%・また、実験では、数値実験を用いて NaaA の有用性を実証する。これが二つ目の貢献である。
%
%・本論文の構成は以下のとおりである。


%自明な解として、他のニューロンの情報をタダで得る解が得られることを防ぐために、
%NaaA はゲーム理論の仕組みを導入し、同じレイヤーに属するニューロン同士を競合学習させることで、neural agent が他のニューロンの価値を正直に申告することを示す。

