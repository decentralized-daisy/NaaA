\section{Neuron as an Agent}
%TODO: Show the figure.
% この章でいうこと
% ・ニューロンをエージェントとみなす
% ・いくつかの前提を提案する
% ・NaaA の social welfare function を最大化することが、外部からのリワード最大化に一致する。

\subsection{Problem Definition}
Suppose there are $N$ agents interacting to an environment. 
%This paper addresses reward distribution problem at multi-agent communication among the agents.
Some agents get reward from the environment, and distribute it to other agents as incentive to give precise information.
The reward is limited so that if an agent distribute $\rho$ reward, the agents' reward subtracted by $\rho$ such as currency.
For the reason, other agents for an agent itself can be interpreted as another environment which 
give a reward $-\rho$ instead of an observation $x$.
We name the another environment as {\em internal environment} and name the original environment as an {\em external environment}.

Our goal is to maximize the discounted cumulative reward which the system will obtain from the external environment.
 %total discounted cumulative reward from the external environment which all the $N$ agents will obtain.
\begin{equation}
%G^\mathrm{ex} = \sum_{i=1}^n \left[ \sum_{k=0}^T \gamma^k R_{i,k+t}^\mathrm{ex} \right],
G = \sum_{i=1}^n \left[ \sum_{t=0}^T \gamma^k R_{it}^\mathrm{ex} \right],
\end{equation}
where $R_{it}^\mathrm{ex}$ is an external reward which $i$-th agent obtains at $t$, and $\gamma \in [0, 1]$ is the discount rate and $T$ is the terminal time.

Similarly to CommNet \citep{sukhbaatar2016learning}, we assume that the communication protocol between the agent is a continuous quantity such as a vector, and the content can be trained by backpropagation.
Hence, we can interpret the multi-agent communication as a huge neural network.
Therefore, we can interpret a unit as an agent without loss of generality.
This framework can single-agent reinforcement learning as well as multi-agent reinforcement learning.

\subsection{The Framework}
A typical artificial neural network is a directed graph $\neuralnet = (\units, \edges)$ among the units.
$\units = \{\unitAt{1}, \dots, \unitAt{N}\}$ is a set of the units. $\edges \subset \units^2$ is a set of edges representing connections between two units.
If $(\unit, \unitAt{j}) \in \edges$, then connection $\unit \rightarrow \unitAt{j}$ holds, indicating that $\unitAt{j}$ observes activation of $\unit$.
We denote activation of the unit $\unit$ at time $t$ as $x_{it} \in \Real$.
Additionally, we designate a set of units which unit $i$ connects to as $\followers = \{j | (\unit, \unitAt{j}) \in \edges \}$ and a set of units which unit $i$ is connected from as $\followees = \{j | (\unitAt{j}, \unit) \in \edges \}$.
We denote $\friends = \followees \cup \followers$.

NaaA interprets $\unit$ as an agent.
Therefore, $\neuralnet$ is a multi-agent system.
An environment for $\unit$ comprises an environment that the multi-agent system itself touches and a 
set of the unit to which $\unit$ directly connects: $\{v_i \in \units | i \in \friends\}$.
We distinguish both environments by naming the former as an external environment, and by naming the latter as an internal environment.
$\unit$ will receive rewards from both environments.
We add the following assumption for characteristics of the $\unit$.
\begin{enumerate}
\renewcommand{\labelenumi}{N\arabic{enumi}:}
\item (Selfishness) 
	The utility which an $\unit$ wants to maximize is its own return (cumulative discounted reward):
	$G_{it} = \sum_{k=0}^T \gamma^k \rewardAt{i,t+k}$.
\item (Conservation) 	% Energy Conservation
	The summation of internal reward over $\units$ equals to 0.
	Hence, the summation of a reward by which $\units$ will receive both an internal and external environment $\reward$ are equivalent to reward $R_t^{\mathrm{ex}}$, which the entire multi-agent system receives from the external environment.
\item (Trade) 
	The $\unit$ receives internal reward $\rho_{jit}$ from $\unitAt{j} \in \units$ in exchange of activation signal $x_i$
	before transferring the signal to the unit. At the same time, $\rho_{jit}$ is subtracted from the reward of $v_j$.
\item (NOOP) 
	$\unit$ has NOOP (no operation), for which the return is $\delta > 0$ as an action.
	With NOOP, the unit inputs nothing and outputs nothing.
\end{enumerate}
In terms of neuroscience,
N1 states that the unit acts as a cell.
N2 and N3 state the distribution of NTF. N4 corresponds to apoptosis.
NOOP is selected when the expected returns of the other actions are non-positive.
In the following, we construct the framework of NaaA from the assumptions.

The social welfare function (total utility of the agents) $G^\mathrm{all}$ is equivalent to the objective function $G$. That is,
\begin{equation}
	G^\mathrm{all} = \sum_{i=1}^n G_{it} = \sum_{i=1}^n \left[ \sum_{k=0}^T \gamma^k R_{it} \right] 
		= \sum_{k=0}^T \left[ \gamma^k \sum_{i=1}^n R_{it} \right].
\end{equation}
From N2, $\sum_{i=1}^n \gamma^k R_{it} = \sum_{i=1}^n R_{it}^{\mathrm{ex}}$ holds. Hence, $G^\mathrm{all} = G$ holds.
%\begin{equation}
%	G^\mathrm{all} = \sum_{k=0}^T \left[ \gamma^k \sum_{i=1}^n R_{it}^{\mathrm{ex}} \right] = G.
%\end{equation}

%================================================================
\subsection{Cumulative Discounted Profit Maximization Framework}
%================================================================
We denote the external reward by which unit $\unit$ receives at time step $t$ as $R_{it}^\mathrm{ex}$, where $\sum_{i=1}^n R_{it}^\mathrm{ex} = R_t^{\mathrm{ex}}$ holds.
From N3, reward $R_{it}$, which $\unit$ receives at $t$ can be written as the following.
\begin{flalign}
	R_{it} = 
	  R_{it}^\mathrm{ex} + \sum_{j \in N^\mathrm{out}_i} \rho_{jit} 
	- \sum_{j \in N^\mathrm{in}_i} \rho_{ijt}.
\end{flalign}
The equation is divided into positive terms and a negative term, we name the former as revenue, and the latter as cost, and denote them respectively as $r_{it} = R_{it}^\mathrm{ex} + \sum_{j \in N^\mathrm{out}_i} \rho_{jit}, \, c_{it} = \sum_{j \in N^\mathrm{in}_i} \rho_{ijt}$.
We name $R_{it}$ as profit.


$\unit$ maximizes the cumulative discounted profit $G_{it}$ represented as
\begin{flalign}
	G_{it}	= \sum_{k=0}^T \gamma^k R_{i, t+k} 
			= \sum_{k=0}^T \gamma^k (r_{i,t+k} - c_{i,t+k})
			&= r_t - c_t + \gamma G_{i,t+1}.
\end{flalign}
$G_{it}$ is unobserved unless the time is reached at the end of the episodes.
Because prediction based on the current value is needed to select the optimal actions, 
we approximate $G_{it}$ with value function $V_i^{\pi_i} (s_{it}) = \Expect{\pi_i}{ G_{it} \mid s_{it}}$ where $s_{it} \in \Observations$.
In this case, the following equation holds.
\begin{flalign} 
		V_i^{\pi_i} (s_{it}) = r_{it} - c_{it} + \gamma V_i^{\pi_i} (s_{i, t+1}),	\label{eq:V}
\end{flalign}
Therefore, we need only consider maximization of revenue, the value function, and cost minimization.
$R_{it} > 0$, i.e., $r_{it} > c_{it}$ indicates that the unit gives the additional value to the obtained data.
The unit acts NOOP because $V_i^{\pi_i} (s_{it}) \leq 0 < \delta$ if $R_{it} \leq 0$ for all $t$ because of N4.

%TODO: verify equation of V



%The design of NaaA is inspired by neuroscience.
%A neuron in a neurocircuit consumes adenosine triphosphate (ATP) supplied from connected astrocytes.
%The astrocyte is a glia cell, which forms the structure of a brain. It supplies fuel from the vessel.
%Because the amount of ATP is constrained, the discarded neuron will become extinct with execution of apoptosis.
%Also, because apoptosis of a neuron is restrained by neurotrophins (NTFs) such as nerve growth factor (NGF) and brain-derived neurotrophic factor (BDNF),
%neurons which can obtain much NTF will live.
%The perspective of interpreting a neuron as an independent living object is known as neural Darwinism \citep{edelman1987neural}.
