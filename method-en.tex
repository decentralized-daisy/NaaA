\section{Proposed Method}
The proposed method, {\em Neuron as an Agent} (NaaA), extends CommNet \citep{sukhbaatar2016learning} to actualize incentive distributions in MARL without TTP based on two key ideas: (i) inter-agent reward distribution and (ii) auction theory.
As we show in Section \ref{sec:naaa}, NaaA actualizes that we can regard even a unit as an agent.

\subsection{Inter-agent Reward Distribution}

%TODO: Show the figure.
% この章でいうこと
% ・ニューロンをエージェントとみなす => ディスカッションでよい
% ・いくつかの前提を提案する
% ・NaaA の social welfare function を最大化することが、外部からのリワード最大化に一致する。

Some agents receive rewards from the environment directly $R_{it}^{\mathrm ex}$, and they distribute these to other agents as incentives for giving precise information.
Rewards are limited, so if an agent distributes $\rho$ rewards, the reward of that agents is reduced by $\rho$ to satisfy the constraint of Eq:(\ref{eq:reward_const}).
For this reason, agents other than a specified agent of interest can be considered a secondary environment for the agent giving 
rewards of $-\rho$ instead of an observation $x$.
This secondary environment was termed the {\em internal environment}, whereas the original environment was called the {\em external environment}.
Similarly to CommNet \citep{sukhbaatar2016learning}, the communication protocol between agents was assumed to be a continuous quantity (such as a vector), the content of which could be trained by backpropagation.
%Under these conditions, it is possible to interpret multi-agent communications as a huge neural network.
%This allowed units to be interpreted as agents without losses of generality.
%This framework can be applied to both single- and multi-agent reinforcement learning.

A communication network among the agents is represented as a directed graph $\neuralnet = (\agents, \edges)$ between agents, where $\agents = \{\agentAt{1}, \dots, \agentAt{N}\}$ is a set of the agents and $\edges \subset \agents^2$ is a set of edges representing the connections between two agents.
If $(\agent, \agentAt{j}) \in \edges$, then connection $\agent \rightarrow \agentAt{j}$ holds, indicating that $\agentAt{j}$ observes the representation of $\agent$.
Here, the representation of agent $\agent$ at time $t$ was denoted as $x_{it} \in \Real$.
Additionally, the set of agents that agent $i$ connects to was designated to be $\followers = \{j | (\agent, \agentAt{j}) \in \edges \}$ and the set of agents that agent $i$ is connected from was $\followees = \{j | (\agentAt{j}, \agent) \in \edges \}$.
Furthermore, $\friends = \followees \cup \followers$.
The following assumptions were added to the $\agent$ characteristics:
\begin{enumerate}
\renewcommand{\labelenumi}{N\arabic{enumi}:}
\item (Selfishness) 
	The utility each agent $\agent$ wants to maximize is its own return (cumulative discounted reward):
	$G_{it} = \sum_{k=0}^T \gamma^k \rewardAt{i,t+k}$.
\item (Conservation) 	% Energy Conservation
	The summation of internal rewards over all $\agents$ equals 0.
	Hence, the summation of rewards which $\agents$ (receive both internal and external environment $\reward$) are equivalent to the reward $R_t^{\mathrm{ex}}$, which the entire multi-agent system receives from the external environment: $\sum_{i=1}^N R_{it} = \sum_{i=1}^N R_{it}^{\mathrm{ex}} = R_t^{\mathrm{ex}}$.
\item (Trade) 
	A agent $\agent$ will receive an internal reward $\rho_{jit}$ from $\agentAt{j} \in \agents$ in exchange for an representation signal $x_i$
	before transferring this signal to the agent. Simultaneously, $\rho_{jit}$ will be subtracted from the reward of $v_j$.
\item (NOOP) 
	$\agent$ can select NOOP (no operation), for which the return is $\delta > 0$, as an action.
	In NOOP, the agent inputs and outputs nothing.
\end{enumerate}

The social welfare function (total utility of the agents) $G^\mathrm{all}$ is equivalent to the objective function $G$. That is,
\begin{equation}
	G^\mathrm{all} = \sum_{i=1}^N G_{it} = \sum_{i=1}^N \left[ \sum_{k=0}^T \gamma^t R_{it} \right] 
		= \sum_{k=0}^T \left[ \gamma^t \sum_{i=1}^N R_{it} \right].
\end{equation}
From N2, $G^\mathrm{all} = G$ holds.
%\begin{equation}
%	G^\mathrm{all} = \sum_{k=0}^T \left[ \gamma^k \sum_{i=1}^n R_{it}^{\mathrm{ex}} \right] = G.
%\end{equation}

%================================================================
\subsubsection{Discounted Cumulative Profit Maximization}
%================================================================
From N3, the reward $R_{it}$ received by $\agent$ at $t$ can be written as:
\begin{flalign}
	R_{it} = R_{it}^\mathrm{ex} + \sum_{j \in N^\mathrm{out}_i} \rho_{jit} 
				- \sum_{j \in N^\mathrm{in}_i} \rho_{ijt},
\end{flalign}
which can be divided into positive and negative terms, where the former is defined as revenue, and the latter as cost. These are respectively denoted as $r_{it} = R_{it}^\mathrm{ex} + \sum_{j \in N^\mathrm{out}_i} \rho_{jit}, \, c_{it} = \sum_{j \in N^\mathrm{in}_i} \rho_{ijt}$.
Here, $R_{it}$ represents profit.


The agent $\agent$ maximized its cumulative discounted profit, $G_{it}$, represented as:
\begin{flalign}
	G_{it}	= \sum_{k=0}^T \gamma^k R_{i, t+k} 
			= \sum_{k=0}^T \gamma^k (r_{i,t+k} - c_{i,t+k})
			&= r_t - c_t + \gamma G_{i,t+1}.
\end{flalign}
$G_{it}$ could not be observed until the end of an episode (the final time).
Because predictions based on current values were needed to select optimal actions, 
$G_{it}$ was approximated with the value function $V_i^{\pi_i} (s_{it}) = \Expect{\pi_i}{ G_{it} \mid s_{it}}$ where $s_{it}$ is a observation for $i$-th agent at time $t$.
Under these conditions, the following equation holds:
\begin{flalign} 
		V_i^{\pi_i} (s_{it}) = r_{it} - c_{it} + \gamma V_i^{\pi_i} (s_{i, t+1}),	\label{eq:V}
\end{flalign}
Thus, herein, we only consider maximization of revenue, the value function, and cost minimization.
The inequality $R_{it} > 0$ (i.e., $r_{it} > c_{it}$) indicates that the agent in question gave additional value to the obtained data.
The agent selected the NOOP action because $V_i^{\pi_i} (s_{it}) \leq 0 < \delta$ if $R_{it} \leq 0$ for all $t$ (because of N4).

%TODO: verify equation of V



%The design of NaaA is inspired by neuroscience.
%A neuron in a neurocircuit consumes adenosine triphosphate (ATP) supplied from connected astrocytes.
%The astrocyte is a glia cell, which forms the structure of a brain. It supplies fuel from the vessel.
%Because the amount of ATP is constrained, the discarded neuron will become extinct with execution of apoptosis.
%Also, because apoptosis of a neuron is restrained by neurotrophins (NTFs) such as nerve growth factor (NGF) and brain-derived neurotrophic factor (BDNF),
%neurons which can obtain much NTF will live.
%The perspective of interpreting a neuron as an independent living object is known as neural Darwinism \citep{edelman1987neural}.

\subsubsection{Social Dilemma}
If we directly optimize Eq:(\ref{eq:V}),
a trivial solution is obtained in which the internal rewards converge at 0, and all agents (excepting agents which directly receive reward from the external environment) select NOOP as their action.
This phenomenon occurs regardless of the network topology $\neuralnet$, as no nodes are incentivized to send payments $\rho_{ijt}$ to other agents.
With this in mind, multi-agent systems must select actions with no information, achieving the equivalent of taking random actions.
For that reason, the external reward $R_t^{\mathrm ex}$ shrinks markedly.
This phenomenon also known as social dilemma in MARL, which is caused from a problem that each agent does not evaluate other agents' value truthfully to maximize their own profit.
We are trying to solve this problem with auction theory in Section \ref{sec:auction}.

\subsection{Auction Theory} \label{sec:auction}
%To maximize the cumulative discounted profit in a framework of NaaA,
%it is important to balance the two contradicting criteria: revenue $r_{it}$ and cost $c_{it}$.

%TODO:
% ・語られていない前提: R_ex は出力ユニットにのみ配分される
% ・語られていない前提: どんなゲームを行うのか（必要なアクションは何か）

%TODO: Negative result. State what is the problem.
%\begin{thm}\label{thm:optimal-bidding-simple}
%$\pi_i$ is the Nash equilibrium if and only if $\rho_{ijt} = 0$ regardless $\neuralnet$
%\end{thm}
%Mechanism design was introduced because, unlike the methods utilized in existing studies \citep{sukhbaatar2016learning}, NaaA assumes that agents are selfish, not cooperative.
%(TODO: Bad writing.)
To make the agents to evaluate other agents' value truthfully, the proposed objective function borrows its idea from the digital goods auction theory \citep{guruswami2005profit}.
In general, an auction theory is a part of mechanism design intended to unveil the true price of goods.
Digital goods auctions are one mechanism developed from auction theory, specifically targeting goods that may be copied without cost such as digital books and music.

\subsubsection{Envy-free Auction}
Although several variations of digital goods auctions exist,
 an envy-free auction \citep{guruswami2005profit} was used here because it required only a simple assumption: equivalent goods have a single simultaneous price.
In NaaA, this can be represented by the following assumption:
\begin{enumerate}
\renewcommand{\labelenumi}{N\arabic{enumi}:}
\setcounter{enumi}{4}
\item (Law of one price)
	If $\rho_{j_1,i,t}, \rho_{j_2,i,t} > 0$, then $\rho_{j_1,i,t} = \rho_{j_2,i,t}$.
\end{enumerate}
The assumption above indicates that $\rho_{jit}$ takes either 0 or a positive value, depending on $i$ at an equal selected time $t$.
Therefore, the positive side was named $\agent$'s {\em price}, and denoted as $q_{it}$.

%TODO: Describe for allocation

The envy-free auction process is shown in the left section of Figure \ref{fig:double}, displaying the negotiation process between one agent sending an representation (defined as a seller), and a group of agents buying the representation (defined as buyers).
First, a buyer places a bid with the agent at a bidding price $b_{jit}$ (\textbf{1}).
Next, the seller selects the optimal price $\opt{q}_{it}$ and allocates the representation (\textbf{2}).
Payment occurs if $b_{ijt}$ exceeds $q_{jt}$.
In this case, $\rho_{jit} = H(b_{jit} - q_{it}) q_{it}$ holds where $H(\cdot)$ is a step function.
For this transaction, the definition $g_{jit} = H(b_{jit} - q_{it})$ holds, and is named {\em allocation}.
After allocation, buyers perform payment: $\rho_{jit} = g_{jit} \opt{q}_{it}$ (\textbf{3}).
The seller sends the representation $x_i$ only to the allocated buyers (\textbf{4}).
Buyers who do not receive the representation approximate $x_i$ with $\Expect{\pi}{x_i}$.
This negotiation is performed at each time step in reinforcement learning.

\subsubsection{Theoretical Analysis}
The sections below discuss the revenue, cost, and value functions based on Eq:(\ref{eq:V}).

%========================================
% 収益
%========================================

\textbf{Revenue}:
The revenue of a agent is given as
\begin{flalign}
	r_{it}  = \sum_{j \in N^\mathrm{out}_i} g_{jit} q_{it} + R^\mathrm{ex}_i 
		= q_i d_{it} + R^\mathrm{ex}_i,
\end{flalign}
where $d_{it} = \sum_{j \in N^\mathrm{out}_i} g_{jit}$ is some number of agents for which the $q_{it}$ bidding price is greater than or equal to $q_{it}$ (designated as the demand).
The equation maximizing $q_{it}$ is designated as the optimal price, denoted as $ \opt{q}_{it} $.
Because $R^\mathrm{ex}_i$ is independent of $q_t$, the optimal price $\opt{q}_{it}$ is given as:
\begin{flalign}
	\opt{q}_{it}  = \argmax_{q \in [0, \infty)} q d_{it}(q).
\end{flalign}
The $r_{it}$ curve is shown on the right side of Figure \ref{fig:double}.

%========================================
% コスト
%========================================
\textbf{Cost}:
Cost is defined as an internal reward that one agent pays to other agents.
It is represented as:
\begin{flalign}
		c_{it} = \sum_{j \in N^\mathrm{in} } g_{ijt} q_{jt} = \vect{g}_{it}^\mathrm{T} \vect{q}_{t},
\end{flalign}
where $\vect{g}_{it} = (g_{i1t}, \dots, g_{iNt})^\T$ and $\vect{q}_{t} = (q_{1t}, \dots, q_{Nt})^\T$.
Although $c_{it}$ itself is minimized when $b_{ijt} = 0$,
this represents a tradeoff with the following value function.

\textbf{Value Function}:
The representation $x_{it}$ depends on input from the agents in $N_i^{\mathrm in}$ affecting the bidding price from agents in $N_i^{\mathrm out}$.
If $b_{ijt}$ is minimized and $b_{ijt} = 0$, the representation purchase fails and the reward obtained by the seller agent from the connected buyer agents is lower in the future.

The effects of the value function were considered for both successful and unsuccessful $v_j$ purchasing cases.
The value function was approximated as a linear function $\vect{g}_{it}$:
\begin{flalign}
		V_i^{\pi_i}(s_{i,t+1}) \approx \vect{o}_{it}^\mathrm{T} \vect{g}_{it} + V_{i,t+1}^0,
\end{flalign}
where %$\vect{o}_{it}$ is an implemented parameter (the difference between two $v_i$ returns) regardless of whether $x_i$ is observed. As
$\vect{o}_{it}$ is equivalent to the cumulative discount value of the counterfactual reward \citep{agogino2006quicr}, it was named {\em counterfactual return}. As $V_{it}^0$ (a constant independent of $\vect{g}_{it}$) is equal to the value function when $v_i$ takes an action without observing $x_1, \dots, x_N$.%, it was named {\em blind value function}.
%This means the cost the unit will pay is $\opt{q}_{it}$ when data purchasing is successful and $o_{it}$ otherwise.

%The value function can be written as an equation using the state-value function $Q(s_{i,t+1}, \vect{g}_{i,t+1})$:
%\begin{flalign}
%	V_i^{\pi_i}(s_{it}) 
%	&= Q_i^{\pi_i}(s_{it}, \vect{g}_{it}) \notag \\
%	&= \sum_{j \in \followees} g_{ijt} (Q_i^{\pi_i} (s_{it}, \vect{e}_j) - Q_i^{\pi_i}(s_{it}, \vect{0})) + Q_i^{\pi_i}(s_{it}, \vect{0}) \notag \\
%	&= \sum_{j \in \followees} g_{ijt} o_{ijt} + Q_i^{\pi_i}(s_{it}, \vect{0}) \notag \\
%	&= \vect{g}_{it}^\T \vect{o}_{it} + Q_i^{\pi_i}(s_{it}, \vect{0}),
%\end{flalign}
%
%We designate $o_{ijt} = Q_i^{\pi_i} (s_{it}, \vect{e}_j) - Q_i^{\pi_i}(s_{it}, \vect{0})$ as the {\em counterfactual return}, 
%which is equivalent to the cumulative discount value of counterfactual reward \citep{agogino2006quicr}.
%That is, the cost the unit will pay is $\opt{q}_{it}$ in success of purchasing data, and $o_{it}$ otherwise.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{img/double.eps}
\caption{
\textbf{Left}: Trade process in an envy-free auction.
\textbf{Right}: Agent price determination curve. Agent revenue is a product of monotonically decreasing demands and prices. The price that maximizes revenue is the optimal price.
}
\label{fig:double}
\end{figure*}


The optimization problem is, therefore, presented below using a state-action value function for $i$-th agent $Q_i(\cdot, \cdot)$:
\begin{flalign}
	%\max_{\vect{b}, q} \Expect{\hat{\vect{q}}_t}{ V_i^{\pi_i}(s_{it}) } = 
		\max_{\vect{a} } Q_i(s_{it}, \vect{a} )  = 
		\max_q q d_{it}(q) - 
		\min_{\vect{b}} \Expect{\hat{\vect{q}}_t}{\vect{g}_{it}(\vect{b})^\T( \hat{\vect{q}}_t - \gamma \vect{o}_{it}  )} + \const,
		\label{Eq:optimization-probem}
\end{flalign}
where $\vect{a} = (\vect{b}, q)$.
Note that $\vect{g}_{it} = H(\vect{b} - \vect{q}_t)$.
The expectation $\Expect{\hat{\vect{q}}_t}{\cdot}$ was taken 
because the asking price $\hat{\vect{q}}_t$ was unknown for $v_i$, except when $\hat{q}_{it}$ and $g_{iit} = 0$.

Then, to identify the bidding price that $b_{it}$ maximizes returns, 
the following theorem holds.

\begin{thm}\label{thm:optimal-bidding}
	(Truthfulness) the optimal bidding price for maximizing returns is $\opt{\vect{b}}_{it} = \gamma \vect{o}_{it}$.
\end{thm}
This proof is shown in the Appendix A.

This implies agents should only consider their counterfactual returns!
When $\gamma = 0$ it is equivalent to a case without auction. Hence, the bidding value is raised if each agent considers their long-time rewards. 
Consequently, when the NaaA mechanism is used agents behave as if performing valuation for other agents, 
and declare values truthfully.

Under these conditions, the following corollary holds:
\begin{coro}\label{coro:optimal-bidding}
	The Nash equilibrium of an envy-free auction is $(\vect{b}_{it}, q_{it})$ is $( \gamma \vect{o}_{it}, \argmax_{q} q d_{it}(q))$.
\end{coro}

The remaining problem is how to predict $\vect{o}_t$.
$Q$-learning was used to predict $\vect{o}_t$ in this paper as the same way as QUICR \cite{agogino2006quicr}.
As $\vect{o}_{it}$ represented the difference between two $Q$s, each $Q$ was approximated.
The state was parameterized using the vector $\vect{s}_t$, 
which contained input and weight.
The $\epsilon$-greedy policy with $Q$-learning typically supposed that discrete actions
Thus the allocation $g_{ijt}$ was employed as an action rather than $\vect{b}_{it}$ and $q_{it}$.
%Although several methods could be applied to this problem (including other RL such as SARSA and A3C \cite{mnih2016asynchronous}),

\paragraph{Algorithm}
The overall algorithm is shown in Algorithm 1.

\begin{algorithm}[t]
\caption{NaaA: inter-agent reward distribution with envy-free auction}
\begin{algorithmic}[1]
	\FOR{ $t=1$ \TO $T$ }
		\STATE Compute a bidding price for every edge: \textbf{for} $(v_j, v_i) \in \edges$ \textbf{do}\
		$b_{ijt} \leftarrow \gamma o_{ijt}$
		%Q^{\pi_i}( \vect{s}_{it}, \vect{e}_j) - Q^{\pi_i}( \vect{s}_{it}, \vect{0})$ 
		\STATE Compute an asking price for every node: \textbf{for} $\agent \in \agents$ \textbf{do}\
		$\opt{q}_{it} \leftarrow \argmax_{q \in [0, \infty)} q d_{it}(q).$
		\FOR{$(v_i, v_j) \in \edges$}
				\STATE Compute allocation: $g_{jit} \leftarrow H(b_{jit} - \opt{q}_{it})$ 
				\STATE Compute the price the agent should pay: $\rho_{jit} \leftarrow g_{jit} \opt{q}_{it}$ 
		\ENDFOR
		\STATE Make a payment: \textbf{for} $\agent \in \agents$ \textbf{do}\
		$R_{it} \leftarrow \sum_{j \in N^\mathrm{out}_i} \rho_{jit} 
				- \sum_{j \in N^\mathrm{in}_i} \rho_{ijt},$
		\STATE Make a shipment: \textbf{for} $\agent \in \agents$ \textbf{do}\
		$\tilde{x}_{ijt} = g_{ijt} x_{ijt} + ( 1 - g_{ijt} ) \bar{x}_{ijt} $

		\FOR{$\agent \in \agents$} 
			\STATE Observe external state $\vect{s}_{it}^{\mathrm ex}$
			\STATE $\vect{s}_{it} \leftarrow (\vect{s}_{it}^{\mathrm ex}, \vect{\tilde{x}}_{it}, \bs{\theta}_i)$,\
				where $\vect{\tilde{x}}_{it} = (\tilde{x}_{i1t}, \dots, \tilde{x}_{int})^\mathrm{T}$ and\
				$\bs{\theta}_i$ is $\agent$'s parameter.
			\STATE Sample action $a_{it}^{\mathrm ex} \sim \pi_i^{\mathrm ex}(\vect{s}_{it})$
			\STATE Receive external reward $R_{it} \leftarrow R_{it} + R_{it}^{\mathrm ex}(a_{it}^{\mathrm ex})$
			%\STATE Update $Q^{\pi_i}$ under the manner of $Q$-learning by calculating the time difference (TD)-error 
			\STATE Update $o_{ijt}$ under the manner of $Q$-learning %by calculating the time difference (TD)-error 
		\ENDFOR
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Neuron as an Agent} \label{sec:naaa}
One benefit of NaaA is that it can be used not only for MARL, but also for network training.
Typical neural network training algorithms such as RMSProp \citep{tieleman2012lecture} and Adam \citep{kingma2014adam} are based on sequential algorithms such as the stochastic gradient descent (SGD).
Therefore, the problem they solve can be interpreted as a problem of updating a state (i.e., weight) to a goal (the minimization of the expected likelihood).

\subsubsection{Adaptive DropConnect}
Learning can be accelerated by applying NaaA to the optimizer.
In this paper, the application of NaaA to SGD was named {\em Adaptive DropConnect} (ADC), 
the finalization of which can be interpreted as a combination of DropConnect \citep{wan2013regularization} and Adaptive DropOut \citep{ba2013adaptive}.
In the subsequent section, ADC is introduced as a potential NaaA application.

ADC uses NaaA for supervised optimization problems with multiple revisions.
In such problems, the first step is the presentation of an input state (such as an image) by the environment. Agents are expected to update their parameters to maximize the rewards presented by a criterion calculator.
Criterion calculators gives batch-likelihoods to agents, representing rewards.
Each agent, a classifier, updates its weights to maximize the reward from the criterion calculator.
These weights are recorded as an internal state.
A heuristic utilizing the absolute value of weight $|w_{ijt}|$ (the
technique used by Adaptive DropOut) was applied as the counterfactual return $o_{ijt}$.
The absolute value of weights was used because it represented the updated amounts for which the magnitude of error of unit outputs was proportional to $|w_{ijt}|$.

This algorithm is presented as Algorithm 2.
Because the algorithm is quite simple, it can be easily implemented and, thus, applied to most general deep learning problems such as 
image recognition, sound recognition, and even deep reinforcement learning.

\begin{algorithm}[t]
\caption{Adaptive DropConnect}
\begin{algorithmic}[1]
	\FOR{ $t=1$ \TO $T$ }
		\STATE Compute a bidding price for every edge: \textbf{for} $(v_j, v_i) \in \edges$ \textbf{do} \
		$b_{ijt} \leftarrow |w_{ijt}|$ 
		\STATE Compute an asking price for every node: \textbf{for} $\agent \in \agents$ \textbf{do} \
		$\opt{q}_{it} \leftarrow \argmax_{q \in [0, \infty)} q d_{it}(q).$
		\FOR{$(v_i, v_j) \in \edges$}
				%\STATE Sample $u$ from a Bernoulli distribution: $u \sim \mathrm{Bernoulli}(\varepsilon)$
				%\STATE Sample $m$ from a Bernoulli distribution: $m \sim \mathrm{Bernoulli}(1/2)$
				\STATE Compute allocation: $g_{jit} \leftarrow H(b_{jit} - \opt{q}_{it})$ 
		\ENDFOR
		\STATE Sample a switching matrix $U_t$ from a Bernoulli distribution: $U_t \sim \mathrm{Bernoulli}(\varepsilon)$
		\STATE Sample the random mask $M_t$ from a Bernoulli distribution: $M_t \sim \mathrm{Bernoulli}(1/2)$
		\STATE Generate the adaptive mask: $M_t' \leftarrow U_t \circ M_t + (1 - U_t) \circ G_{ijt}$ 
		\STATE Compute $\vect{h}_t$ for making a shipment:
			$\vect{h}_t \leftarrow (M_t' \circ W_t) \vect{x}_t + \vect{b}_t$
		\STATE Update $W_t$ and $\vect{b}_t$ by backpropagation.
	\ENDFOR
\end{algorithmic}
\end{algorithm}

%========================================================================
% 【論旨】
% - Q-learning と \epsilon-greedy 方策による強化学習で最適化をする
%	 - 状態を入力と重みを用いてパラメトライズする
%	 - 行動として allocation \g を用いる
%	 - 報酬は profit: revenue と cost の差
%    - つまり、NaaA では結果的に全体としてパフォーマンスが向上するようコネクションを最適化する
% 	 - これはランダムにエッジを落とす dropconnect の拡張であり、より精度向上ができると考えられる。
% - NaaA に基づくネットワーク最適化を adaptive dropconnect と名付ける
%	 - 類似の事例として、過去に提案されている adaptive dropout はこれをより一般化した話
%	 - $\epsilon=0$ の場合には dropconnect と完全に等しくなる
% - Adaptive dropconnect は、強化学習以外にも応用可能
% 	 - 強化学習以外の場合は、報酬として正解に基づく 0/1 の情報を用いて、$gamma = 0$ とする。
% 	 - 実装上は層を入れ替えるだけなので簡単
% - アルゴリズム
%========================================================================

%$\vect{o}_t$ のみ用いた greedy な方策を用いると、
%方策として、$\epsilon$-greedy を用いた
%We use $\epslion-greedy$
%We show that pre $Q$ lead us t
%我々は $\epslion-greedy$ における探索は dropconnect であるため、
%adaptive dropconnect に等しいことを示す。
%
%$\epsilon$
%
%次に、adaptive dropconnect について述べる。
%
%Value of the value function $V(s_{i,t+1})$ depends on $s_{i,t+1}$.
%As we already defined, the internal environment of $v_i$ is a set of connected units,
%and the output of units affect to evaluation from the units, namely, weight of edges.
%As the learning rule of a typical artificial neural network obeys to law of Hebb, 
%the reward becomes lower because weight of unit which do not contribute
%the accuracy of output becomes lower.

