\section{Related Work}
%=================================
% Contents
% - 強化学習
% - NaaA: Dec-POMDP
% - 要素技術
% 　- メカデザ（ゲーム理論と強化学習）
% 　- Dropconnect
%=================================

%受け取る報酬に相関があるマルチエージェントの設定を意図的に作り出すことで、目的を達成するニューラルネットワークの学習のためのアイデアが最近注目されている。

特定の問題を解くために、マルチエージェントの設定を意図的に作り、ニューラルネットワークを学習するアイデアが注目され始めている。
generative adversarial nets (GAN) \citep{goodfellow2014generative} は、
generator と discriminator という報酬が相反する 2 つのエージェント間の競合ゲームを通し、
両者の個別最適であるナッシュ均衡として、真の生成分布を得ることが目的である。 %望ましい結果である真の分布を生成することが目的である。
ゲーム理論では、全体の報酬を最大化する結果のことをパレート最適と呼ぶが、
一般にナッシュ均衡がパレート最適に収束することは保証されておらず、両者の不一致はジレンマと呼ばれる。
ジレンマが存在するか否かは報酬設計に依存しているため、最適な報酬設計を通してジレンマを解消する方法が研究されている。
これは逆ゲーム理論(inverse game theory)としても知られるメカニズムデザイン\citep{myerson1983mechanism}である。
メカニズムデザインは、オークション \citep{vickrey1961counterspeculation} やマッチング \citep{gale1962college} に応用されている。
GAN、そして本研究で提案する NaaA は、メカニズムデザインの一つである。
NaaA では、メカニズムデザインの一つである digital goods auction \citep{guruswami2005profit} を応用することで、
ナッシュ均衡として、ユニットが系全体のリターンを最大化することを示す。

NaaA は、複数のニューロンをエージェントとして処理することから、Dec-POMDP (decentralized POMDP) のクラスに属する。
Dec-POMDP は、POMDP 環境に複数エージェントを想定している強化学習のクラスであり、様々な研究課題がある。
一つはコミュニケーションである。
CommNet \citep{sukhbaatar2016learning} はニューラルネットワークに属するユニットが他のユニットのトポロジーに対して agnostic である性質を応用し、マルチエージェントのコミュニケーションの訓練にバックプロパゲーションを応用している。
もう一つは報酬の分配である。CommNet は全エージェントに、系が受け取った報酬を分配することを前提としているが、
エネルギーや通貨のように全体量が限られている報酬の場合は分割が困難である。
QUICR-learning \citep{agogino2006quicr} では、エージェント $i$ が reward $R(a_t)$ の代わりに、
そのエージェントがある行動 $a_{it}$ をとった場合 $a_t$ と取らなかった場合 $a_t-a_{it}$ の差、
counterfactual reward $R(a_t) - R(a_t - a_{it})$ の cumulative discount summation を最大化している。
COMA \citep{foerster2017counterfactual} は、actor-critic において critic が共通しており、actor がマルチエージェントであるという actor-critic の仕組みを考え、それぞれの actor が counterfactual reward を最大化するような仕組みを考えている。
本研究は、これら二つの問題を統一的に扱い、エージェントが自身の counterfactual return を最大化するように行動する仕組みを提案する。

%?gアプローチがあるが、ここでは三つを紹介する。
%?g一つは、学習の効率を高めるために、同一のモデルに従う複数のエージェントを用いて探索を行う方法であり、Gorilla \citep{nair2015massively} , A3C \citep{mnih2016asynchronous} などで採用されている。
%?g二つ目は、アクチュエーターを増やすことによって行動の量を増やす方法であり、サッカーゲーム \citep{kalyanakrishnan2006half} やテレビゲーム \citep{tampuu2017multiagent} で行われている。
%?g三つ目は、センサーを増やすことによって観測の量を増やす方法であり、自動運転\citep{sukhbaatar2016learning}やセンサーネットワーク\citep{fox2000probabilistic}などで行われている。
%?g本研究は、観測の重要性に注目しているため、三つ目のケースをスコープとする。
%Dec-POMDP では、信頼度割り当て問題の解決が重要になる。

%なお、メカニズムデザインやオークション理論、マッチング理論は経済学において注目されている分野であり、
%いずれも提唱者がそれぞれ 2007 年、1996 年、2012 年にノーベル経済学賞を受賞している。

%代表的なメカニズムデザインの成果はオークション理論である。
%GAN や今回提案する NaaA もそれに含まれる。
%NaaA の最適化はメカニズムデザインの一つであり、すべてのエージェントが協力関係になり、全体最適化を行うような工夫を行う。
%マルチエージェント環境においてエージェントの報酬が互いに独立でない場合、ゲーム理論を用いた解決が施される。

% ANN
%\cite{rumelhart1985learning} らによって人工ニューラルネットワーク(ANN)の訓練方法として導入されたバックプロパゲーションは、
%バックプロパゲーションは、チェーンルールによって重みやバイアスなどのパラメータの最適化を行うため、ユニットは直接接続している以外のユニットについて agnostic である。
%これは非常に beneficial な性質であり、ユニットが全体の大域的なストラクチャーを知る必要はないため、中央集権的なモデルを有する必要がないことを意味する。
%CommNet \citep{sukhbaatar2016learning} はこの性質を応用し、マルチエージェントのコミュニケーションの訓練にバックプロパゲーションを応用している。
%一方、CommNet はすべてのエージェントが同一の報酬を最大化することを前提としているため、貢献度に応じた報酬の配分が実現されない。
%本研究は報酬の配分を対象としている。

TODO: Dropconnect



% Dec-POMDP

% ジレンマ問題

% Dropconnect (Pruning の話と同様に)
% ここに Dec-POMDP を書く

%強化学習には、その構成要素である環境とエージェントの性質に応じていくつかの問題のクラスが存在する。
%最も代表的なものは DQN が依拠している Markov decision process であるが、
%MDP はすべての状態が観測可能であることを前提としているため、視野など単一のエージェントの観測範囲が限られている実世界の問題には fit しない。
%そのため partially observed MDP (PODMP) が注目されており、DRQN に拡張する研究などが注目されている。
%さらに、単一のエージェントでの観測範囲や行動の制約から、マルチエージェントで強化学習問題を解くという試みもされており、
%これは Dec-POMDP \citep{} として定式化されている。

%一般に distributed AI は、P2P システムを中心に研究されている\citep{}。
%集合知などに応用することが可能であるとされている \citep{}
%ニューラルネットワークも分散システムという見方は可能であり、
%進化論を応用したニューラルダーウィニズムという仮説も提唱されている\cite{}。

\iffalse
現在成功している深層強化学習のモデルの多くは、単一のエージェントが環境の観測から認知、行動決定といった一連のプロセスを担う。
DQN \citep{mnih2015human,silver2016mastering} は、Atari のスクリーン系列から最適な行動を決定したり、AlphaGo のモジュールとして囲碁の盤面から勝利に最も近い一手を選ぶ。
DDPG \citep{lillicrap2015continuous} は物理空間において摩擦や重力係数などの条件を考慮した多関節の制御を実現する。
単一のエージェントを用いて強化学習を解くという試みは、人間の持つ身体性のアナロジーから考えると一見して妥当であるように思えるが、現実世界は open world であり、単一のエージェントが完全に情報が観測することが難しい。
そのためマルチエージェントによるアプローチが求められている。
%A3C \citep{mnih2016asynchronous} 

深層強化学習を POMDP 環境に適用する研究はいくつか行われている。
Deep Recurrent Q-Network (DRQN) \citep{sorokin2015deep} は、
隠れマルコフ連鎖を想定し、リカレントニューラルネットワーク(RNN)を用いて真の状態を推定している。
%他にも、エレベーター制御\citep{crites1998elevator}、センサーネットワーク\citep{fox2000probabilistic}、ロボットサッカー\citep{stone1998towards}などがマルチエージェントによって解かれている（修正中：最新の研究をサーベイ）。



マルチエージェントで強化学習の問題を解決する場合には、信頼度割り当て問題の解決が重要になる。
そこで、エージェントの信頼度を、そのエージェントがいた場合と、
いなかったと仮定した場合の差として定量化する研究が行われている。
QUICR-learning \citep{agogino2006quicr} では、エージェント $i$ が reward $R(a_t)$ の代わりに、
そのエージェントがある行動 $a_{it}$ をとった場合 $a_t$ と取らなかった場合 $a_t-a_{ti}$ の差、
counterfactual reward $R(a_t) - R(a_t - a_{it})$ の cumulative discount summation を最大化している。
COMA \citep{foerster2017counterfactual} は、actor-critic において critic が共通しており、actor がマルチエージェントであるという actor-critic の仕組みを考え、それぞれの actor が counterfactual reward を最大化するような仕組みを考えている。

マルチエージェントによる観測および認知の枠組みは、センサー処理の分野ではエッジコンピューティング\citep{bonomi2012fog}としても知られている。
エッジコンピューティングは分散環境を前提とした信号処理のモデルであり、一つの処理系がすべてのデータを処理するのではなく、複数のセンサーの情報を一つのエッジサーバが集約し、複数のエッジサーバが次元削減したデータをデータセンターに送るという階層的な構造をしている。
本研究は、実際に IoT の環境で本研究が適用されるケースを想定している。
報酬はビットコインなどの決済手段によって送金が可能であるため、
Web 全体でスケーラブルなモデルが実現できる可能性がある。

NaaA の設計は、神経科学からヒントを得ている。
神経回路に含まれるニューロンは一つの細胞であるため、エネルギーを消費する。
通常の細胞と同様に酸素や ATP がエネルギー源となり、これらはニューロンと接続した、アストロサイトから供給される。
アストロサイトは脳の構造を支えるグリア細胞の一種であり、血管からニューロンへの栄養供給を行う。
エネルギー量は有限であるため、不要なニューロンはアポトーシスによって死滅する。
アポトーシスは NGF (nerve growth factor), BDNF (brain derived neurofactor) などの神経栄養因子(neurotorophin; NTF)によって制御されるため、より多くの NTF を獲得できたニューロンが生存する。
各神経細胞を独立した生物として捉える見方はニューラルダーウィズム\citep{edelman1987neural}と呼ばれる。

神経科学モデルから見ても、ニューロンは一つの細胞として機能するため、エネルギーやニューロトロフィンを消費する、アポトーシスを起こすなど自律的な振る舞いをする。
このため、本質的には複数のレイヤー、複数のユニットを異なるオーナーが作り、フォワード/バックプロパゲーションのコミュニケーションのみで、全体最適化を行うことができる。その場合は貢献度に応じた報酬を分け合う必要がある。
我々の提案するフレームワーク、NaaA はこうした仕組みをモデリングしている。
\fi
