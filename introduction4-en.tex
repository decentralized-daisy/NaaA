\section{Introduction}
Recent successful results of deep reinforcement learning (DRL) in TV games \citep{mnih2015human} and board games \citep{silver2016mastering} are supported by capability of a deep neural network to obtain good state representation by abstracting high-dimensional input. %such as a screen and a board face.
Unlike these artificial environments, applying DRL to industry such as mobility, finance and agriculture requires way to observe state representation in a partially observed environment.
In addition to predicting unobserved state which reports reasonable performance in partially observed environment such as a first-person shooting game \citep{dosovitskiy2016learning},
multi-agent reinforcement learning (MARL) is an emerging topic to observe wider range of state with communication.
Several researchers proposed methods to learn appropriate communication as a cooperative setting \citep{sukhbaatar2016learning}.
 %is a method for cooperative MARL to learn multi-agent communication among agents with common objective.
%the real environment cannot be completely observed by single agent.

What is unsolved by these communication methods in MARL is reward distribution for configuration in which agents are not cooperative but selfish.
Considering an environment such as Web, these agents will be developed by different people and companies with different objective.
Hence, they are not incentivesed to develop an agent without an appropriate reward distribution.
As the recent work \citep{sukhbaatar2016learning} considers whole multi-agent system with communication as a neural network,
this discussion for incentive is reduced to consider a unit as an agent ultimately.
Therefore, we address the following question.
% equivalent to asking the following question.
%Eventually, the problem is dealt with by answering the following question.
%MARL allows each agents made of a single unit.
%CommNet considers an agent as an neuron, and train the communication between the agents with back propagation.

\begin{center}
{\em Will reinforcement learning work even if we consider each unit as a selfish agent?}
\end{center}

%To answer the research question. We should consider sequential communication structure in which 
%agents give reward for other agents.
%Previous studies for reward distribution such as QUICR-Learning and COMA assumes that shallow action 
%deep communication.
%Reward distribution 
%This is a trade-off between 

%==========================
% 例えば、はじめに research question を挙げているのはわかりやすくていいのですが、そこから NaaA へは論理的に繋がらないです。この research question に答えようとする研究は多分過去にもいっぱいあるはずなので、それらの研究の中でどこまでできていて何ができていないのか？できていないことの中でどの課題に取り組むことが大事なのか？その課題の解決はなぜ難しいのか？その課題をどういうアイデアで解決しようとしているのか？なぜそのアイデアで解決できるのか？ということを、論理的に必然性を持って繋げて書くと NaaA を考えたくなる気持ちになると思います。現状だと、なにか面白そうなこと言っているけど突拍子もなく出てきたので評価しようがないものになってる気がします。Section 3 でも、はじめに神経科学の話を持ってきているのですが、なぜこういうニューロンのモデルをこの問題に適用したくなったのかが共感できないまま話が進むので、なんかわからんという気持ちになりました。
%==========================

The contribution of this paper is that we propose {\em Neuron as an Agent} (NaaA) as a novel framework for RL, and its optimization method.
NaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.
In the reward design of NaaA, a unit distributes its received reward as cost to other input units in order to observe their activation.
Consequently, the actual reward is profit, defined as the difference between inflow (received reward) and outflow (paid cost).
In the setting, the economic metaphor can be introduced: profit is the balance of revenue and cost. 
%There are several point to discuss for the reward distribution:
%"How much the reward is appropriate per agent" and "How to obligate agents to distribute reward."
%The one is known as credit assignment problem in multi-agent reinforcement learning (MARL), and
%the another one ks also known as a treatment of social dilemma.
%Recently, emerging technology of blockchain enabled us to distribute reward to agents with cryptocurrency such as bitcoin and ethereum.
The source of reward is from reward which the actuator obtain by obtaining good state representation from the environment.

%====================================================================================
% ここはこのままでよい
This paper is organized as presented below.
First, showing the optimization of NaaA, this report describes the negative result that the return (cumulative discounted reward) decreases if we naively consider units as agents.
As a solution to this difficulty, we introduce a mechanism of auction which applies game theory.
As a theoretical result, we demonstrate that the agents maximize their {\em counterfactual return} in Nash equilibrium.
The counterfactual return is that by which we extend counterfactual reward, the criterion proposed for multi-agent reward distribution problem \citep{agogino2006quicr}, along a long time axis.

Subsequently, we present that maximizing counterfactual return leads the model to learning optimal topology between the units with respect to supervised learning as well as RL.
In addition, we propose {\em adaptive dropconnect}, a natural extension of dropconnect \citep{wan2013regularization}.
Adaptive dropconnect combines dropconnect, which randomly masks the topology, with an adaptive algorithm, which prunes connections with less counterfactual return with higher probability.
It uses $\varepsilon$-greedy as a policy, and is equivalent to dropconnect in the case of $\varepsilon = 1$. It is equivalent to counterfactual return maximization, which constructs the topology deterministically in the case of $\varepsilon = 0$.

Finally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.
Specifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.
%====================================================================================

Although considering all the units as agents might be simplistic at first glance, it has a wider applicable area.
From the perspective of optimization for a single neural network, it can be applied to pruning by optimizing the topology.
Furthermore, introducing the concept of reward distribution divides the single neural network to numerous autonomous parts.
It enables us not only to address sensor placing problem in IoT for partially observed Markov decision process (POMDP): arbitrary incentivized participants can join the framework.

%We focus on DRL in the multi-agent setting.
%Deep Deterministic Policy Gradient (DDPG) \citep{lillicrap2015continuous} realizes the multiple-join control considering conditions such as friction and gravity factors in a physical space.
%The applicability of DRL is becoming wider year by year. Reasonable performance is reported for 3D games such as Doom \citep{dosovitskiy2016learning}.
%Applying DRL to industry such as mobility, finance and agriculture as people expected, requires curious way to observe environment.

%The problem can be solved by two approaches.
%The one is predicting transition of unobserved state by neural network,
%which achieved by an agent with a recurrent model such as a recurrent neural network to explore
%the environment actively to obtain information.
%The another one is to prepare multi-agent environment to observe environment indirectly by communication.
