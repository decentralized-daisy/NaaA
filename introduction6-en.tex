\section{Introduction}

%===========================================================================================
% Motivation
%===========================================================================================
After intelligence with reinforcement learning beat humans \citep{tesauro1995temporal,mnih2015human,silver2016mastering}, reinforcement learning has been expected to be applied into industry such as stock trade, autonomous cars, smart grid and IoT. 
In a world which realized the industrial application in the future, various types of companies will own their agents to improve their revenue.
The situation can be regarded as one that each agent are independently solving problems of partially observed Markov decision process (POMDP).

Although agents in the companies are closed to maximize their own reward, if the agents exchange their own information each other, 
entire revenue will be improved more than individuals.
Using economic metaphor, similarly to a supply chain of diamond from a company which collects raw material, one which processes it to one which sells it, if we can realize representation learning in the each layers as social division, a single company can yield revenue more than one by itself. %in the case the company does it by itself.
Thus, this paper aims to realize a society in which stakeholders which can have a conflict of interest trade their own information.

We regard the situation as communication in multi-agent reinforcement learning (MARL), addressed by several existing methods such as R/DIAL \citep{foerster2016learning} and CommNet \citep{sukhbaatar2016learning}.
CommNet is a state-of-the-art of MARL which considers communication among agents, 
and the feature is learning among agents with backpropagation.

In the case when we are trying to consider MARL in which different stakeholders make different agents which communicate each other, it needs design of incentive distribution (e.g., monetary payment) and a framework without {\em trusted third party} (TTP).
TTP is a neutral administrator which assumes distribution of reward for all the participants, supposed implicitly by most of existing literatures with regard to MARL \citep{agogino2006quicr,foerster2016learning,sukhbaatar2016learning}.
While TTP is required to be neutral against all the participants,
several configuration of peer-to-peer trade such as inter-industry and -country trade cannot prepare TTP.
If untrusted third party assumes reward distribution, it can undesirably make reward for partial participants higher than necessity.

To the best of our knowledge, no existing literatures discuss the reward distribution on the configuration above.
Since CommNet assumes an environment which distributes a uniform reward to all the agents, 
in the case distributing limited reward in supply such as money, it causes {\em Tragedy of the Commons} \citep{lloyd1833two} in which contributing agents' reward will reduce due to participant of free riders.
Although there are several MARL methods which distributes reward depends on their contribution such as QUICR \citep{agogino2006quicr} and COMA \citep{sukhbaatar2016learning}, they suppose the existence of TTP, and hence it cannot be applied into our situation.

%===========================================================================================
% Objective
%===========================================================================================

Our proposed method, {\em Neuron as an Agent} (NaaA) extends CommNet to realize incentive distribution
in MARL without TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory.
The reason why we introduce auction theory is inter-agent reward distribution is insufficient for optimization.
An agent in NaaA maximizes {\em profit}, difference between reward which it receives and cost which it redistributes to other agents.
If we optimize the framework naively, we obtain a trivial solution that agents make their cost zero to maximize the profit.
Then, NaaA employs game design with auction theory to keep cost being smaller than necessity.
As a theoretical result, we show that an agent autonomously evaluates {\em counterfactual return} as other agent's value.
Counterfactual return equals to discounted cumulative sum of counterfactual reward \citep{agogino2006quicr} which QUICR and COMA distribute.
NaaA realizes reward distribution which make it pareto improvement more than inter-agent reward distribution.

NaaA enables us to trade of representation in peer-to-peer and regard a unit in a neural network as an agent ultimately.
As NaaA can regard a unit as an agent without loss of generality indeed, this paper uses the setting.
We illustrate the concept proposed method in Fig. 1 (TBD).

In the experiment, we use an environment which extends ViZDoom \citep{kempka2016vizdoom}, a POMDP environemnt, to MARL.
We put two agents in the environment.
The one is a cameraman who send information, and the another one is a main player to defeat enemies with a gun.
We confirm that the cameraman learns cooperative action to send information in dead angle, behind of main player, and outperform CommNet in score.

% Organization
The remaining part of this paper is organized as follows. 
First, we describe the two key ideas: inter-agent reward distribution and auction theory. 
After introducing related works, we show the experimental result in ViZDoom.
Next, we show Adapritve DropConnect as a further application. Then we perform discussion and conclude this paper.
