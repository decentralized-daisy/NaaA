\section{Related Work}
% TODO: マルチエージェントの分配についての話を書く

% 既存研究ではどういうアプローチでこの問題を扱っている？
% MARL の研究を基本的には書いていく

% 具体的な内容: CommNet, Forester
% QUICR, COMA
% Adaptive DropConnect

%Deep multi-agent reinforcement learning (DMARL) is emerging topic of deep learning and reinforcement learning.
%Several researcher attept to solve the topic in environement such as Starcraft and Soccor game.
%DMARL works good for inperfect information.

NaaA belongs to a class of partially observable stochastic game (POSG) \citep{hansen2004dynamic} because it processes multiple units as agents.
POSG, a class of reinforcement learning with multiple agents in a POMDP environment, presents several research issues, one of which is communication.
CommNet \citep{sukhbaatar2016learning}, which exploits the characteristics of a unit that is agnostic to the topology of other units, employs backpropagation to train multi-agent communication.
Another one is credit assignment.
Instead of reward $R(a_t)$ of an agent $i$ for actions at $t$ $a_t$, 
QUICR-learning \citep{agogino2006quicr} maximizes counterfactual reward $R(a_t) - R(a_t - a_{it})$, the difference in the case of the agent $i$ takes an action $a_{it}$ ($a_t$) and not ($a_t-a_{it}$).
COMA \citep{foerster2017counterfactual} also maximizes counterfactual rewards in an actor--critic setting.
In the setting, all actors have common critics, which improves both actors and critics with time difference (TD)-error of a counterfactual reward.
This paper unifies both issues: communication and credit assignment.
The main proposal is a framework to manage the agents to maximize the {\em counterfactual return}, the extended counterfactual reward along the time axis.


%Agent-based computational economics (ACE) is a topic of computatinoal economics.
% MARL に経済学を応用することができる。
% ・経済学で仮定している経済人はエージェントシステムに等しい。
% ・MARL が直面している報酬の配分問題は経済学の問題としてモデリングできる
% ・エージェント間の取引が最大化することができる

% TODO ここに経済学のゲーム理論が応用できる話を書く
Training a neural network with a multi-agent game is an emerging methodology.
%Generative adversarial nets (GAN) \citep{goodfellow2014generative} have the goal of obtaining true generative distribution as a Nash equilibrium of a competitive game that includes two agents with contradictory rewards: a generator and a discriminator. 
%In game theory, social welfare. %the outcome maximizing overall reward is named Pareto optimality.
%converge to Pareto optimality. 
Nash equilibrium is not guaranteed to maximizing overall reward, and the difference is designated as a social dilemma.
Because the existence of a dilemma depends on the reward design, methods to resolve dilemmas with good reward design are being investigated: mechanism design \citep{myerson1983mechanism} is also known as inverse game theory.
Mechanism design is applied to auctions \citep{vickrey1961counterspeculation} and matching \citep{gale1962college}.
NaaA is outcomes from mechanism design.
NaaA applies a digital goods auction \citep{guruswami2005profit} to reinforcement learning with a multi-agent neural network, 
to obtain a maximized return by units as a Nash equilibrium.

Adaptive DropConnect (ADC), which we propose in a later part of this paper, extends DropConnect \citep{wan2013regularization}, a regularization technique.
The idea of ADC (instead of dropping each connection between units in constant probability, using skew probability correlated to the absolute value of weights) is eventually closer to Adaptive DropOut \citep{ba2013adaptive}, although the derivation differs.
The adjective ``adaptive'' is added with respect to the method.
Optimizing the neural network with RL was investigated by \cite{andrychowicz2016learning}.
In contrast to their methods, which use recurrent neural network (RNN) and which therefore have difficult implementation,
our method is RNN-free and forms as a layer. For those reasons, its implementation is simple and fast. Moreover, it has a wide area of applicability.

% TODO: 調べて書く。他に同様の話を実現するニューラルネットワークがあるかどうか
