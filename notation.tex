\subsection{Notation}
We give a brief explanation of a DQN \citep{mnih2015human} whose 
objective function of minimizing the square of time difference (TD)-error will be 
combined with the computational cost of evaluating the objective 
function to obtain our CATDQN. 

The objective with a DQN is to learn control policies of an agent in a 
game by RL \citep{sutton1998reinforcement}, where the interaction of 
the agent in the game (or environment) is modeled as a Markov decision 
process (MDP). An MDP $(\S, \A, \R, \Trans, \gamma)$ consists of a set of 
states $\S$, set of actions $\A$, immediate reward function $\R: 
\S\times\A \rightarrow \Real$, transition probability $\Trans: \S 
\times \A \times \S \rightarrow [0,1]$, (i.e., it assigns a 
probability of moving from one state to another with a particular 
action), and discount rate $\gamma \in [0, 1)$ . 
For a computer game, these variables respectively correspond 
to the screen and other states of the game, output of a controller, 
scores, and rules of the game.

